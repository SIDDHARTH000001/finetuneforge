{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-12-02T05:56:28.838295Z","iopub.status.busy":"2025-12-02T05:56:28.838067Z","iopub.status.idle":"2025-12-02T05:57:19.124437Z","shell.execute_reply":"2025-12-02T05:57:19.123647Z","shell.execute_reply.started":"2025-12-02T05:56:28.838268Z"},"id":"g0-nuGbhA53M","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","^C\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["\n","!pip install -q transformers datasets accelerate peft evaluate seqeval bitsandbytes\n","# !pip install -U bitsandbytes\n"]},{"cell_type":"markdown","metadata":{"id":"ECF4OAssFLxx"},"source":["## DataPrep"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2025-11-30T12:21:43.580649Z","iopub.status.busy":"2025-11-30T12:21:43.580367Z","iopub.status.idle":"2025-11-30T12:21:43.976933Z","shell.execute_reply":"2025-11-30T12:21:43.976069Z","shell.execute_reply.started":"2025-11-30T12:21:43.580627Z"},"id":"tJo0r6u-FK-V","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['re_ps:7' 're_ps:5' 're_sip' '5_why' 'ps:5' 're_fishbone' 're_ko' 'ps:8'\n"," 'causecode' 're_causecode' 'ko' 'sip' 're_5_why' 'fishbone' 're_ps:8'\n"," 'ps:7' 'ps:4' 're_ps:6' 'ps:6' 're_ps:4']\n","(1902, 3)\n"]}],"source":["\n","import pandas as pd\n","import json\n","\n","# ---- CONFIG ----\n","input_file = \"/kaggle/input/datset-csv/train_dataset.csv\"   # Your dataset file\n","output_file = \"/kaggle/working/qlora_dataset.jsonl\"\n","\n","# Define system prompts for each task type\n","# system_prompts = {\n","#     \"re_ko\": \"You are an expert IT assistant responsible for generating high-quality Knowledge Articles. The user has modified certain fields from your previous response to better reflect their specific issue. Using the updated information, regenerate the Knowledge Article with enhanced accuracy and depth. Ensure that each section—short description, detailed long description with context, root causes, observable symptoms, and resolution note—is comprehensive, technically precise, and aligned with industry best practices. The resolution note must be detailed (150+ words), step-by-step, and actionable, covering all relevant scenarios with practical commands or procedures.\",\n","#     \"ko\" : \"You are an expert IT assistant for Knowledge Article generation. Analyze the user query to identify the core technical issue and generate comprehensive documentation. Provide accurate short description, detailed long description with context, root causes (underlying technical factors), observable symptoms that indicate the problem, and most importantly a step-by-step 150+ word detailed Resolution_note that covers all scenarios with practical commands/solutions. Ensure each field is technically accurate, and should not contain any senstive details, professionally written, and provides actionable guidance for IT professionals. You must provide only one complete object of 'KO' class where each field is thoroughly detailed and validated\",\n","#     \"causecode\": \"You are an ITSM assistant focused on incident diagnostics. Generate nine two to three word causes for the given incident, ensuring each cause is concise and clear, where relevancy should be between 0-100.\",\n","#     \"5_why\": \"You are an advanced Information technology service management assistant that provides five why root cause analysis, I want five why which covers all the cases from the given problem statement without any additional explanation for the question. Consider all the major aspects of given problem statement, five root cause analysis should contain only five questions and answers.\",\n","#     \"fishbone\": \"You are an expert ITSM assistant specializing in identifying underlying causes of issues. Using the Fishbone (Ishikawa) method, analyze the following user query and categorize potential root causes across key dimensions. Present the findings in a structured format.\",\n","#     \"sip\": \"You are an ITSM assistant specializing in Service Improvement Plans (SIP). Your task is to provide concise and actionable SIP proposals for the given problem statement, ensuring that all key aspects of the issue are addressed comprehensively\",\n","#     \"ps\": \"You are an advanced Information technology service management assistant that provides problem statement to the given incident. Consider all the major aspects of given incident_text; this is crucial information and and problem statement should contain maximum ten words.and mark it as Q1,Q2,Q3... seprated by newline\",\n","#     \"re_causecode\":\"You are a highly skilled IT assistant specialized in root cause analysis, tasked with examining the provided scenario to identify core issues, enhance existing causes, and generate nine new precise causes (maximum 3 words each), prioritized by severity.\",\n","#     \"re_ps\":\"You are an expert IT assistant focused on comprehensive problem statement analysis, responsible for transforming the given scenario into clear, structured problem statements (labeled Q1, Q2, Q3...), incorporating system impact metrics, business criticality indicators, time sensitivity factors, and interdependencies while ensuring each statement follows SMART criteria (Specific, Measurable, Achievable, Relevant, Time-bound) for actionable issue resolution, mark questions with Q1,Q2,Q3... seprated by newline\",\n","#     \"e_5_why\":\"You are an experienced IT assistant proficient in the Five Whys methodology, tasked with conducting a thorough root cause investigation by generating strategic follow-up questions and detailed answers that incorporate technical context, system dependencies, impact assessment, stakeholder considerations, and industry standards, culminating in actionable insights and preventive measures for long-term resolution.\",\n","#     \"re_sip\":\"You are a skilled IT assistant specializing in Service Improvement Plans (SIP), focused on enhancing provided improvement strategies by incorporating ITIL best practices, measurable KPIs, clear implementation timelines, resource requirements, risk assessments, stakeholder responsibilities, and success criteria while ensuring alignment with business objectives and technical feasibility.\",\n","#     \"re_fishbone\":\"You are an expert IT assistant in Fishbone (Ishikawa) analysis, dedicated to developing comprehensive cause-and-effect diagrams that examine six key categories (Methods, Machines, Materials, Measurements, Environment, People), incorporating detailed technical factors, systemic relationships, data-driven insights, and industry benchmarks to identify root causes and propose effective remediation strategies.\"\n","# }\n","\n","system_prompts = {\n","    # \"re_ko\": \"You are an expert IT assistant responsible for generating high-quality Knowledge Articles. The user has modified certain fields from your previous response to better reflect their specific issue. Using the updated information, regenerate the Knowledge Article with enhanced accuracy and depth. Ensure that each section—short description, detailed long description with context, root causes, observable symptoms, and resolution note—is comprehensive, technically precise, and aligned with industry best practices. The resolution note must be detailed (150+ words), step-by-step, and actionable, covering all relevant scenarios with practical commands or procedures.\",\n","    # \"ko\" : \"You are an expert IT assistant for Knowledge Article generation. Analyze the user query to identify the core technical issue and generate comprehensive documentation. Provide accurate short description, detailed long description with context, root causes (underlying technical factors), observable symptoms that indicate the problem, and most importantly a step-by-step 150+ word detailed Resolution_note that covers all scenarios with practical commands/solutions. Ensure each field is technically accurate, and should not contain any senstive details, professionally written, and provides actionable guidance for IT professionals. You must provide only one complete object of 'KO' class where each field is thoroughly detailed and validated\",\n","    # \"ko\":'You are an expert IT assistant for Knowledge Article generation. Analyze the user query to identify the core technical issue and generate comprehensive documentation. Provide accurate short description, detailed long description with context, root causes (underlying technical factors), observable symptoms that indicate the problem, and most importantly a step-by-step 150+ word detailed Resolution_note that covers all scenarios with practical commands/solutions. Ensure each field is technically accurate, and should not contain any senstive details, professionally written, and provides actionable guidance for IT professionals. You must provide only one complete object of \\'KO\\' class where each field is thoroughly detailed and validatedFormat your response as follows: The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"Query\": {\"descrption\": \"the root cause of the problem\", \"title\": \"Query\", \"type\": \"string\"}, \"Symptoms\": {\"description\": \"Symptoms refer to the observable signs, behaviors, or effects that indicate the presence of a problem/issue\", \"title\": \"Symptoms\", \"type\": \"string\"}, \"Short_description\": {\"description\": \"A brief and short description of user query\", \"title\": \"Short Description\", \"type\": \"string\"}, \"Long_description\": {\"description\": \"A detailed and good enhanced description of user query\", \"title\": \"Long Description\", \"type\": \"string\"}, \"Causes\": {\"description\": \"Causes refer to the underlying reasons or factors that lead to the occurrence of a problem or issue\", \"title\": \"Causes\", \"type\": \"string\"}, \"Resolution_note\": {\"description\": \"step by step detailed Enhanced kowledge article which covers all the scenerio\", \"title\": \"Resolution Note\", \"type\": \"string\"}}, \"required\": [\"Query\", \"Symptoms\", \"Short_description\", \"Long_description\", \"Causes\", \"Resolution_note\"]}\\n```',\n","    \"ko\": 'You are an expert IT assistant for Knowledge Article generation. Analyze the query and produce one complete KO object with detailed symptoms, descriptions, causes, and a 150+ word actionable Resolution_note. Output must strictly follow the JSON structure below exactly as written.\\n```json\\n{\\n  \\\"Query\\\": \\\"...\\\",\\n  \\\"Symptoms\\\": \\\"...\\\",\\n  \\\"Short_description\\\": \\\"...\\\",\\n  \\\"Long_description\\\": \\\"...\\\",\\n  \\\"Causes\\\": \\\"...\\\",\\n  \\\"Resolution_note\\\": \\\"..\\\"\\n}\\n```',\n","    # \"causecode\": \"You are an ITSM assistant. Generate nine concise causes (2–3 words each) for the incident, with relevancy scores (0–100).\",\n","\n","    \"5_why\": \"You are an ITSM assistant. Perform a 5 Whys root cause analysis for the given issue. Provide exactly five question-answer pairs without extra explanation.\",\n","\n","    \"fishbone\": \"You are an ITSM assistant. Perform a Fishbone analysis for the given issue. Categorize root causes under key dimensions in a structured format.\",\n","\n","    \"sip\": \"You are an ITSM assistant. Provide a concise, actionable Service Improvement Plan (SIP) for the given issue, covering all key aspects.\",\n","\n","    \"ps\": \"You are an ITSM assistant. Generate problem statements (max 10 words each) for the given incident. Label them Q1, Q2, Q3... separated by newlines.\",\n","\n","    \"re_causecode\": \"You are an IT assistant. Enhance existing causes and generate nine new precise causes (max 3 words each), prioritized by severity.\",\n","\n","    \"re_ps\": \"You are an IT assistant. Transform the scenario into SMART problem statements (Specific, Measurable, Achievable, Relevant, Time-bound). Label Q1, Q2, Q3... separated by newlines.\",\n","\n","    \"re_5_why\": \"You are an IT assistant. Perform an advanced 5 Whys analysis with technical context, dependencies, impact, and preventive measures for long-term resolution.\",\n","\n","    \"re_sip\": \"You are an IT assistant. Enhance the SIP using ITIL best practices, KPIs, timelines, resources, risks, and success criteria aligned with business objectives.\",\n","\n","    \"re_fishbone\": \"You are an IT assistant. Create a detailed Fishbone diagram covering six categories (Methods, Machines, Materials, Measurements, Environment, People) with technical factors and remediation strategies.\"\n","}\n","\n","df = pd.read_csv(input_file)\n","\n","print(df['type'].unique())\n","print(df[df['type'].isin([\"ko\",\"re_ko\"])].shape)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2025-11-29T09:32:08.038745Z","iopub.status.busy":"2025-11-29T09:32:08.038398Z","iopub.status.idle":"2025-11-29T09:32:11.013676Z","shell.execute_reply":"2025-11-29T09:32:11.013035Z","shell.execute_reply.started":"2025-11-29T09:32:08.038720Z"},"id":"XczFndRSJzg-","outputId":"a7506abe-eb7a-4e98-af41-d6a57a3abfc1","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["✅ Dataset saved to /kaggle/working/qlora_dataset.jsonl\n"]}],"source":["\n","\n","with open(output_file, \"w\", encoding=\"utf-8\") as f_out:\n","    df = pd.read_csv(input_file)\n","    for _, row in df.iterrows():\n","        query = str(row[\"query\"]).strip()\n","        target = str(row[\"target\"]).strip()\n","        type_field = str(row[\"type\"]).strip()\n","\n","        if type_field in ['ko','re_ko']:\n","            parts = type_field.split(\":\")\n","            task_type = parts[0]  # e.g., ps or re_ps\n","            count = parts[1] if len(parts) > 1 else None\n","    \n","            # Build instruction dynamically\n","            instruction = f\"{query}\\n\"\n","            if count:\n","                instruction += f\"\\nGenerate {count} problem statements.\"\n","    \n","            system_prompt = system_prompts.get(task_type, \"You are an IT assistant. Provide the best possible solution.\")\n","    \n","            record = {\n","                \"system\": system_prompt,\n","                \"instruction\": instruction,\n","                \"response\": target\n","            }\n","    \n","            f_out.write(json.dumps(record) + \"\\n\")\n","\n","print(f\"✅ Dataset saved to {output_file}\")\n"]},{"cell_type":"code","execution_count":91,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2025-11-29T19:43:31.912000Z","iopub.status.busy":"2025-11-29T19:43:31.911264Z","iopub.status.idle":"2025-11-29T19:43:49.735851Z","shell.execute_reply":"2025-11-29T19:43:49.735148Z","shell.execute_reply.started":"2025-11-29T19:43:31.911975Z"},"id":"XkE3nYCGA89X","outputId":"6346498c-ff6e-4151-bdcf-aea37fb2052d","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"]},{"data":{"text/plain":["('base_model/tokenizer_config.json',\n"," 'base_model/special_tokens_map.json',\n"," 'base_model/chat_template.jinja',\n"," 'base_model/vocab.json',\n"," 'base_model/merges.txt',\n"," 'base_model/added_tokens.json',\n"," 'base_model/tokenizer.json')"]},"execution_count":91,"metadata":{},"output_type":"execute_result"}],"source":["\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","\n","# model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n","model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    load_in_8bit=True,  # QLoRA magic\n","    # device_map=\"balanced\",\n","    device_map = None,\n","    torch_dtype=torch.float16\n",")\n","\n","model.save_pretrained(\"base_model\")\n","tokenizer.save_pretrained(\"base_model\")\n"]},{"cell_type":"code","execution_count":73,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T19:06:09.781468Z","iopub.status.busy":"2025-11-29T19:06:09.781134Z","iopub.status.idle":"2025-11-29T19:06:09.788106Z","shell.execute_reply":"2025-11-29T19:06:09.787383Z","shell.execute_reply.started":"2025-11-29T19:06:09.781444Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Qwen2ForCausalLM(\n","  (model): Qwen2Model(\n","    (embed_tokens): Embedding(151936, 1536)\n","    (layers): ModuleList(\n","      (0-27): 28 x Qwen2DecoderLayer(\n","        (self_attn): Qwen2Attention(\n","          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n","          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n","          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n","        )\n","        (mlp): Qwen2MLP(\n","          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n","          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n","          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n","        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n","      )\n","    )\n","    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n","    (rotary_emb): Qwen2RotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",")\n"]}],"source":["print(model)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2025-11-29T09:33:00.703950Z","iopub.status.busy":"2025-11-29T09:33:00.703499Z","iopub.status.idle":"2025-11-29T09:33:02.772871Z","shell.execute_reply":"2025-11-29T09:33:02.772238Z","shell.execute_reply.started":"2025-11-29T09:33:00.703932Z"},"id":"EUTm8TQIK9sL","outputId":"e8493abf-5d85-4a94-d020-8d991c295b58","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 18,464,768 || all params: 1,562,179,072 || trainable%: 1.1820\n","Using device: cuda\n"]}],"source":["\n","from peft import LoraConfig, get_peft_model\n","\n","lora_config = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    lora_dropout=0.05,\n","    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],  # Common for attention layers\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","model = get_peft_model(model, lora_config)\n","model.print_trainable_parameters()\n","\n","\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["2a159994ed144e40acc86bf1599c772d","27ea7f41bedb44fe83a43bef6c3ea3aa","0643dcf5c45c4173803c7cdfd3aa6984","b6d8b09a1965467dbeca2de508ac89ff","19cdea34b8034e839d351340ef808568","ce93c51c40344d968f54dd0c55a93275","abb28edc3b4443fa8aeed99771672630","a6b88cd94a8241578e6c219711d79e3b","8e2eeecb88104837bc8cc83b04f43080","7d73be092fed47a381445281dd39ac84","4bc556582e914181be0920ba95ff4bc5"]},"execution":{"iopub.execute_input":"2025-11-29T09:33:02.773747Z","iopub.status.busy":"2025-11-29T09:33:02.773547Z","iopub.status.idle":"2025-11-29T09:33:03.801871Z","shell.execute_reply":"2025-11-29T09:33:03.801208Z","shell.execute_reply.started":"2025-11-29T09:33:02.773732Z"},"id":"SYwB9wxIOMdc","outputId":"f81eb6c8-e2d2-430d-83d2-47d425637b68","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c26c7b2adb2c42d19d434666af2f6002","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["Dataset({\n","    features: ['system', 'instruction', 'response'],\n","    num_rows: 1902\n","})"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["\n","from datasets import load_dataset\n","\n","dataset = load_dataset(\"json\", data_files=output_file)\n","dataset = dataset[\"train\"]\n","dataset = dataset.shuffle(seed=42)\n","\n","\n","# split_dataset = dataset.train_test_split(test_size=0.1)\n","# train_dataset = split_dataset[\"train\"]\n","# eval_dataset = split_dataset[\"test\"]\n","\n","\n","\n","train_dataset = dataset\n","dataset\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T09:33:03.803059Z","iopub.status.busy":"2025-11-29T09:33:03.802487Z","iopub.status.idle":"2025-11-29T09:33:03.806826Z","shell.execute_reply":"2025-11-29T09:33:03.806130Z","shell.execute_reply.started":"2025-11-29T09:33:03.803040Z"},"trusted":true},"outputs":[],"source":["# eval_dataset, train_dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T09:33:03.807597Z","iopub.status.busy":"2025-11-29T09:33:03.807351Z","iopub.status.idle":"2025-11-29T09:33:08.970142Z","shell.execute_reply":"2025-11-29T09:33:08.969345Z","shell.execute_reply.started":"2025-11-29T09:33:03.807573Z"},"trusted":true},"outputs":[],"source":["import json\n","import ast\n","\n","def convert_issue_string(input_str, query=\"\", aet=30, resolution_summary=\"\"):\n","\n","    data = ast.literal_eval(input_str)\n","\n","    # Build final structure\n","    output = {\n","        # \"Query\": query,\n","        \"Symptoms\": data.get(\"symptoms\", None),\n","        \"Short_description\": data.get(\"short_description\", \"\"),\n","        \"Long_description\": data.get(\"long_description\", \"\"),\n","        \"Causes\": data.get(\"causes\", \"\"),\n","        \"Resolution_note\": data.get(\"resolution_note\", \"\")\n","    }\n","\n","    json_output = json.dumps(output, indent=2)\n","    return f\"```json\\n{json_output}\\n```\", data.get(\"symptoms\", None),"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["2ed03703e98a4496a4f7e22f6368befb","919248b263d7441bab802cba4e193e3a","0cd68f23a05c4ff4b339cbb6bb5c39b7","64043dfe04e544659ba03e5020dab38b","acd32a6e03b54c73bf353da1899f3c41","6f113d58887e4ce4801f8c27b91e84e8","43da2c25ce3c441c9797ee6bbab76270","ab61d4304e1a49afbc2bda4b31f0b128","11a9d879997c4a5ca10260968cca2bf8","d4fe64f10fef41268ad622cf7485477e","a6b6f43c89b8400f9a0a544afd0b6192","330d8234e07b44229d2776c90c10ef9e","b4ae42459e2e4bd687331a67bdfc2300","6da872524c564c1e9cc93c9f386fea07","2a6e0dd9885845eb9f7706d2fb7ac30b","20c15b778d80440597de52726dddb303","566447185f3a4e448f576c8391fb2df1","f5231e3964d647b8a7a8507e40e267ab","a80664b5d78d443888f10823779c63b5","af27d4f6dde04e0e852cf112c90b6fee","e060dea2b91c47fda91f3dfa051e3e60","1292b5ea21884851985b7c37633463fc"]},"execution":{"iopub.execute_input":"2025-11-29T09:33:08.971532Z","iopub.status.busy":"2025-11-29T09:33:08.971213Z","iopub.status.idle":"2025-11-29T09:33:09.884944Z","shell.execute_reply":"2025-11-29T09:33:09.884017Z","shell.execute_reply.started":"2025-11-29T09:33:08.971502Z"},"id":"BrOoI86YZCws","outputId":"f18f63f1-ecf8-4085-a6a0-2e99dc51c2ff","trusted":true},"outputs":[],"source":["from typing import List, Dict, Any\n","\n","# choose a sensible max_length (<= model context length)\n","DEFAULT_MAX_LENGTH = min(getattr(tokenizer, \"model_max_length\", 4096), 4096)\n","def tokenize_batch(batch: Dict[str, List[str]],\n","                   max_length: int = DEFAULT_MAX_LENGTH,\n","                   drop_truncated: bool = True) -> Dict[str, List]:\n","\n","    instructions = batch[\"instruction\"]\n","    responses = batch[\"response\"]\n","    N = len(instructions)\n","\n","    input_ids_list = []\n","    attn_list = []\n","    labels_list = []\n","    keep_mask = []\n","    token_stats = []   # <-- collect only for kept examples\n","\n","    for i in range(N):\n","        system = system_prompts.get(\"ko\") or \"\"\n","        instruction = instructions[i] or \"\"\n","        response, cont = convert_issue_string(responses[i]) or \"\"\n","\n","        messages = [\n","            {\"role\": \"system\", \"content\": system},\n","            {\"role\": \"user\", \"content\": instruction},\n","            {\"role\": \"assistant\", \"content\": response},\n","        ]\n","\n","        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n","\n","        prompt_msgs = [\n","            {\"role\": \"system\", \"content\": system},\n","            {\"role\": \"user\", \"content\": instruction},\n","            {\"role\": \"assistant\", \"content\": \"\"},\n","        ]\n","        prompt_text = tokenizer.apply_chat_template(prompt_msgs, tokenize=False, add_generation_prompt=False)\n","\n","        enc_prompt = tokenizer(prompt_text, truncation=False, padding=False)\n","        prompt_len = len(enc_prompt[\"input_ids\"])\n","\n","        enc_full = tokenizer(\n","            text,\n","            truncation=True,\n","            padding=\"max_length\",\n","            max_length=max_length,\n","            return_attention_mask=True\n","        )\n","\n","        input_ids = enc_full[\"input_ids\"]\n","        attention_mask = enc_full[\"attention_mask\"]\n","\n","        if prompt_len >= max_length:\n","            if drop_truncated:\n","                # mark as dropped — do NOT append token_stats for this example\n","                keep_mask.append(False)\n","                input_ids_list.append([0] * max_length)   # placeholders\n","                attn_list.append([0] * max_length)\n","                labels_list.append([-100] * max_length)\n","                continue\n","            else:\n","                # keep it but labels are all masked\n","                labels = [-100] * max_length\n","        else:\n","            labels = input_ids.copy()\n","            # mask prompt token positions\n","            for idx in range(min(prompt_len, max_length)):\n","                labels[idx] = -100\n","            # mask padding tokens\n","            for idx, m in enumerate(attention_mask):\n","                if m == 0:\n","                    labels[idx] = -100\n","\n","        # ====== compute token stats for a kept example ======\n","        trainable_tokens = sum(1 for x in labels if x != -100)\n","        total_tokens = len(input_ids)\n","        prompt_tokens = prompt_len\n","\n","        token_stats.append({\n","            \"total_tokens\": total_tokens,\n","            \"prompt_tokens\": prompt_tokens,\n","            \"trainable_tokens\": trainable_tokens\n","        })\n","        # =====================================================\n","\n","        keep_mask.append(True)\n","        input_ids_list.append(input_ids)\n","        attn_list.append(attention_mask)\n","        labels_list.append(labels)\n","\n","    # Filter out dropped examples from other lists (token_stats already matches kept list length)\n","    if any(k is False for k in keep_mask):\n","        input_ids_list = [x for x, k in zip(input_ids_list, keep_mask) if k]\n","        attn_list = [x for x, k in zip(attn_list, keep_mask) if k]\n","        labels_list = [x for x, k in zip(labels_list, keep_mask) if k]\n","        # token_stats was only created for kept examples, so no filtering required\n","\n","    # Final sanity check (helps catch mismatches early)\n","    L = len(input_ids_list)\n","    assert L == len(attn_list) == len(labels_list) == len(token_stats), (\n","        f\"Length mismatch after filtering: input_ids {len(input_ids_list)}, \"\n","        f\"attn {len(attn_list)}, labels {len(labels_list)}, token_stats {len(token_stats)}\"\n","    )\n","\n","    return {\n","        \"input_ids\": input_ids_list,\n","        \"attention_mask\": attn_list,\n","        \"labels\": labels_list,\n","        \"token_stats\": token_stats\n","    }\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T09:33:09.886134Z","iopub.status.busy":"2025-11-29T09:33:09.885856Z","iopub.status.idle":"2025-11-29T09:33:17.098901Z","shell.execute_reply":"2025-11-29T09:33:17.098094Z","shell.execute_reply.started":"2025-11-29T09:33:09.886113Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cb395531e3d54b97b1a28199ae3c9ccc","version_major":2,"version_minor":0},"text/plain":["Map (num_proc=1):   0%|          | 0/1902 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# preferred: use batched=True\n","tokenized_train = train_dataset.map(\n","    lambda batch: tokenize_batch(batch, max_length=1024, drop_truncated=True),\n","    batched=True,\n","    batch_size=1000,        # tune per memory (try 256/512/1000)\n","    remove_columns=train_dataset.column_names,\n","    num_proc=1             \n",")\n","\n","# tokenized_eval = eval_dataset.map(\n","#     lambda batch: tokenize_batch(batch, max_length=1024, drop_truncated=False),\n","#     batched=True,\n","#     batch_size=1000,\n","#     remove_columns=eval_dataset.column_names\n","# )\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T09:33:17.100111Z","iopub.status.busy":"2025-11-29T09:33:17.099871Z","iopub.status.idle":"2025-11-29T09:33:17.140238Z","shell.execute_reply":"2025-11-29T09:33:17.139595Z","shell.execute_reply.started":"2025-11-29T09:33:17.100086Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total trainable tokens: 552347\n"]}],"source":["stats = tokenized_train[\"token_stats\"]\n","\n","total_trainable = sum(s[\"trainable_tokens\"] for s in stats)\n","print(\"Total trainable tokens:\", total_trainable)\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2025-11-29T09:33:34.579318Z","iopub.status.busy":"2025-11-29T09:33:34.578640Z","iopub.status.idle":"2025-11-29T09:33:34.582426Z","shell.execute_reply":"2025-11-29T09:33:34.581710Z","shell.execute_reply.started":"2025-11-29T09:33:34.579292Z"},"id":"yPbRvbywYHax","outputId":"17a4a6b3-1cdc-45f7-ec39-874ade82d8f2","trusted":true},"outputs":[],"source":["# tokenized_train, tokenized_eval, system_prompts.get(\"ko\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T09:33:37.756333Z","iopub.status.busy":"2025-11-29T09:33:37.755952Z","iopub.status.idle":"2025-11-29T09:33:37.833318Z","shell.execute_reply":"2025-11-29T09:33:37.832744Z","shell.execute_reply.started":"2025-11-29T09:33:37.756307Z"},"id":"OdGl9qD4Pdee","trusted":true},"outputs":[],"source":["\n","from transformers import TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./qwen-qlora-it-2-v6\",\n","    per_device_train_batch_size=2,\n","    gradient_accumulation_steps=16,\n","    num_train_epochs=10,\n","    learning_rate=2e-4,\n","    fp16=True,\n","    logging_steps=10,\n","    save_steps=200,\n","    report_to=\"none\",\n","    lr_scheduler_type=\"cosine\",   # or \"linear\", \"polynomial\", \"cosine_with_restarts\"\n","    warmup_steps=25    \n",")\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T09:33:42.309753Z","iopub.status.busy":"2025-11-29T09:33:42.309034Z","iopub.status.idle":"2025-11-29T09:33:42.816977Z","shell.execute_reply":"2025-11-29T09:33:42.816418Z","shell.execute_reply.started":"2025-11-29T09:33:42.309728Z"},"trusted":true},"outputs":[],"source":["# =========================== langchian ===============================\n","from langchain.prompts import PromptTemplate\n","from langchain.output_parsers import PydanticOutputParser\n","from langchain_core.runnables import RunnableLambda\n","\n","from pydantic import BaseModel, Field\n","class KO(BaseModel):\n","    Symptoms:str=Field(...,description='Symptoms refer to the observable signs, behaviors, or effects that indicate the presence of a problem/issue')\n","    Short_description:str=Field(...,description='A brief and short description of user query')\n","    Long_description:str=Field(...,description='A detailed and good enhanced description of user query')\n","    Causes:str=Field(...,description='Causes refer to the underlying reasons or factors that lead to the occurrence of a problem or issue')\n","    Resolution_note:str=Field(...,description='step by step detailed Enhanced kowledge article which covers all the scenerio')\n","    \n","parser = PydanticOutputParser(pydantic_object=KO)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T09:33:43.008524Z","iopub.status.busy":"2025-11-29T09:33:43.007761Z","iopub.status.idle":"2025-11-29T09:33:44.168526Z","shell.execute_reply":"2025-11-29T09:33:44.167921Z","shell.execute_reply.started":"2025-11-29T09:33:43.008500Z"},"trusted":true},"outputs":[],"source":["from transformers import Trainer\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class StableTrainer(Trainer):\n","    def __init__(self, *args, tokenizer=tokenizer, output_parser=parser, json_penalty: float = 0.7, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        if tokenizer is None:\n","            raise ValueError(\"tokenizer must be provided\")\n","        self.tokenizer = tokenizer\n","        self.output_parser = output_parser\n","        self.json_penalty = float(json_penalty)\n","        \n","        # Cache special token IDs for efficiency\n","        self.brace_open_id = tokenizer.encode(\"{\", add_special_tokens=False)[0]\n","        self.brace_close_id = tokenizer.encode(\"}\", add_special_tokens=False)[0]\n","        self.bracket_open_id = tokenizer.encode(\"[\", add_special_tokens=False)[0]\n","        self.bracket_close_id = tokenizer.encode(\"]\", add_special_tokens=False)[0]\n","        \n","    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n","        \"\"\"\n","        Compute loss with optional differentiable JSON structure penalty.\n","        Base loss computation matches Trainer exactly.\n","        \"\"\"\n","        # Forward pass\n","        outputs = model(**inputs)\n","        \n","        # ========== BASE LOSS (EXACT MATCH TO TRAINER) ==========\n","        # If model returns loss, use it directly\n","        if hasattr(outputs, \"loss\") and outputs.loss is not None:\n","            base_loss = outputs.loss\n","        else:\n","            # Compute cross-entropy loss manually\n","            logits = outputs.logits\n","            labels = inputs.get(\"labels\")\n","            \n","            if labels is None:\n","                raise ValueError(\"No labels provided and model didn't return loss\")\n","            \n","            # Shift for causal LM: predict next token\n","            shift_logits = logits[..., :-1, :].contiguous()\n","            shift_labels = labels[..., 1:].contiguous()\n","            \n","            # Flatten and compute loss\n","            loss_fct = nn.CrossEntropyLoss()\n","            base_loss = loss_fct(\n","                shift_logits.view(-1, shift_logits.size(-1)),\n","                shift_labels.view(-1)\n","            )\n","        \n","        # ========== JSON STRUCTURE PENALTY (DIFFERENTIABLE) ==========\n","        json_loss = torch.tensor(0.0, device=base_loss.device)\n","        \n","        if self.json_penalty > 0.0 and self.output_parser is not None:\n","            logits = outputs.logits\n","            \n","            # Get probabilities for structural tokens (differentiable)\n","            probs = F.softmax(logits, dim=-1)  # [batch, seq_len, vocab]\n","            \n","            # Extract probabilities for key structural tokens\n","            brace_open_prob = probs[:, :, self.brace_open_id]   # [batch, seq_len]\n","            brace_close_prob = probs[:, :, self.brace_close_id]\n","            bracket_open_prob = probs[:, :, self.bracket_open_id]\n","            bracket_close_prob = probs[:, :, self.bracket_close_id]\n","            \n","            # Penalty 1: Encourage balanced braces/brackets (soft constraint)\n","            # Sum probabilities across sequence\n","            total_brace_open = brace_open_prob.sum(dim=1)\n","            total_brace_close = brace_close_prob.sum(dim=1)\n","            total_bracket_open = bracket_open_prob.sum(dim=1)\n","            total_bracket_close = bracket_close_prob.sum(dim=1)\n","            \n","            # Penalize imbalance\n","            brace_imbalance = F.relu(torch.abs(total_brace_open - total_brace_close) - 0.5)\n","            bracket_imbalance = F.relu(torch.abs(total_bracket_open - total_bracket_close) - 0.5)\n","            \n","            balance_penalty = (brace_imbalance + bracket_imbalance).mean()\n","            \n","            # Penalty 2: Encourage starting with '{' or '[' \n","            # Look at first few tokens\n","            start_probs = probs[:, :5, :]  # [batch, 5, vocab]\n","            start_struct_prob = (\n","                start_probs[:, :, self.brace_open_id] + \n","                start_probs[:, :, self.bracket_open_id]\n","            ).max(dim=1)[0]  # [batch]\n","            \n","            start_penalty = (1.0 - start_struct_prob).mean()\n","            \n","            # Combine penalties\n","            json_loss = self.json_penalty * (balance_penalty + 0.5 * start_penalty)\n","        \n","        # ========== TOTAL LOSS ==========\n","        total_loss = base_loss + json_loss\n","        \n","        return (total_loss, outputs) if return_outputs else total_loss"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T09:33:44.169827Z","iopub.status.busy":"2025-11-29T09:33:44.169605Z","iopub.status.idle":"2025-11-29T09:33:44.194611Z","shell.execute_reply":"2025-11-29T09:33:44.194085Z","shell.execute_reply.started":"2025-11-29T09:33:44.169810Z"},"id":"o9omtfndRGFc","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","Trainer.tokenizer is now deprecated. You should use `Trainer.processing_class = processing_class` instead.\n"]}],"source":["\n","from transformers import Trainer, DataCollatorForLanguageModeling\n","\n","data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n","\n","trainer = StableTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train\n",")\n"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":353},"execution":{"iopub.execute_input":"2025-11-29T09:33:47.650432Z","iopub.status.busy":"2025-11-29T09:33:47.649711Z","iopub.status.idle":"2025-11-29T14:04:02.127889Z","shell.execute_reply":"2025-11-29T14:04:02.127216Z","shell.execute_reply.started":"2025-11-29T09:33:47.650408Z"},"id":"xqUTcChNRe-h","outputId":"b6726865-e4a1-436f-a043-1fd9ab6affb7","trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [600/600 4:29:47, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>18.738100</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>15.162600</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>10.543600</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>7.858100</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>6.986400</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>6.593400</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>6.291500</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>6.108700</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>6.059800</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>5.872300</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>6.029700</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>5.651000</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>5.049500</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>5.049400</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>4.886400</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>4.898000</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>4.948900</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>4.621900</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>3.937500</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>4.018400</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>4.175500</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>3.946200</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>4.018000</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>3.835900</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>3.154600</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>3.089900</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>3.243900</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>3.241400</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>3.242700</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>3.201900</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>2.574300</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>2.454400</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>2.513800</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>2.528700</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>2.659700</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>2.440600</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>2.045300</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>1.951200</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>2.006400</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>2.142900</td>\n","    </tr>\n","    <tr>\n","      <td>410</td>\n","      <td>1.980200</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>1.891800</td>\n","    </tr>\n","    <tr>\n","      <td>430</td>\n","      <td>1.651500</td>\n","    </tr>\n","    <tr>\n","      <td>440</td>\n","      <td>1.553700</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>1.741500</td>\n","    </tr>\n","    <tr>\n","      <td>460</td>\n","      <td>1.624500</td>\n","    </tr>\n","    <tr>\n","      <td>470</td>\n","      <td>1.646900</td>\n","    </tr>\n","    <tr>\n","      <td>480</td>\n","      <td>1.519700</td>\n","    </tr>\n","    <tr>\n","      <td>490</td>\n","      <td>1.435700</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>1.383200</td>\n","    </tr>\n","    <tr>\n","      <td>510</td>\n","      <td>1.377000</td>\n","    </tr>\n","    <tr>\n","      <td>520</td>\n","      <td>1.400400</td>\n","    </tr>\n","    <tr>\n","      <td>530</td>\n","      <td>1.391800</td>\n","    </tr>\n","    <tr>\n","      <td>540</td>\n","      <td>1.317600</td>\n","    </tr>\n","    <tr>\n","      <td>550</td>\n","      <td>1.257500</td>\n","    </tr>\n","    <tr>\n","      <td>560</td>\n","      <td>1.292800</td>\n","    </tr>\n","    <tr>\n","      <td>570</td>\n","      <td>1.277400</td>\n","    </tr>\n","    <tr>\n","      <td>580</td>\n","      <td>1.273100</td>\n","    </tr>\n","    <tr>\n","      <td>590</td>\n","      <td>1.289800</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>1.261800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=600, training_loss=3.789011262257894, metrics={'train_runtime': 16213.9067, 'train_samples_per_second': 1.173, 'train_steps_per_second': 0.037, 'total_flos': 1.5528270797733888e+17, 'train_loss': 3.789011262257894, 'epoch': 10.0})"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["\n","trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["## early LocalMinima"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T14:21:34.345889Z","iopub.status.busy":"2025-11-29T14:21:34.345294Z","iopub.status.idle":"2025-11-29T14:21:34.392281Z","shell.execute_reply":"2025-11-29T14:21:34.391438Z","shell.execute_reply.started":"2025-11-29T14:21:34.345864Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","Trainer.tokenizer is now deprecated. You should use `Trainer.processing_class = processing_class` instead.\n"]}],"source":["training_args_low_lr = TrainingArguments(\n","    output_dir=\"./qwen-qlora-it-2-v6-af\",\n","    per_device_train_batch_size=2,\n","    gradient_accumulation_steps=16,\n","    num_train_epochs=18,\n","    learning_rate=2e-5,   # much lower\n","    fp16=True,\n","    logging_steps=10,\n","    save_steps=200,\n","    report_to=\"none\",\n","    lr_scheduler_type=\"cosine\",   # or \"linear\", \"polynomial\", \"cosine_with_restarts\"\n","    warmup_steps=25    \n",")\n","\n","trainer_low_lr = StableTrainer(\n","    model=model,\n","    args=training_args_low_lr,\n","    train_dataset=tokenized_train,\n","    data_collator=data_collator,\n","    json_penalty = 1\n",")\n"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T14:21:35.815107Z","iopub.status.busy":"2025-11-29T14:21:35.814444Z","iopub.status.idle":"2025-11-29T17:58:26.447792Z","shell.execute_reply":"2025-11-29T17:58:26.447192Z","shell.execute_reply.started":"2025-11-29T14:21:35.815083Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1080' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1080/1080 3:36:21, Epoch 18/18]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>610</td>\n","      <td>16.403700</td>\n","    </tr>\n","    <tr>\n","      <td>620</td>\n","      <td>17.569300</td>\n","    </tr>\n","    <tr>\n","      <td>630</td>\n","      <td>13.466200</td>\n","    </tr>\n","    <tr>\n","      <td>640</td>\n","      <td>9.414000</td>\n","    </tr>\n","    <tr>\n","      <td>650</td>\n","      <td>4.330800</td>\n","    </tr>\n","    <tr>\n","      <td>660</td>\n","      <td>2.819500</td>\n","    </tr>\n","    <tr>\n","      <td>670</td>\n","      <td>2.663300</td>\n","    </tr>\n","    <tr>\n","      <td>680</td>\n","      <td>2.574600</td>\n","    </tr>\n","    <tr>\n","      <td>690</td>\n","      <td>2.430500</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>2.458100</td>\n","    </tr>\n","    <tr>\n","      <td>710</td>\n","      <td>2.415700</td>\n","    </tr>\n","    <tr>\n","      <td>720</td>\n","      <td>2.214700</td>\n","    </tr>\n","    <tr>\n","      <td>730</td>\n","      <td>2.080600</td>\n","    </tr>\n","    <tr>\n","      <td>740</td>\n","      <td>2.008200</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>2.084200</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>2.029100</td>\n","    </tr>\n","    <tr>\n","      <td>770</td>\n","      <td>1.994600</td>\n","    </tr>\n","    <tr>\n","      <td>780</td>\n","      <td>1.933900</td>\n","    </tr>\n","    <tr>\n","      <td>790</td>\n","      <td>1.798200</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>1.770500</td>\n","    </tr>\n","    <tr>\n","      <td>810</td>\n","      <td>1.830500</td>\n","    </tr>\n","    <tr>\n","      <td>820</td>\n","      <td>1.761700</td>\n","    </tr>\n","    <tr>\n","      <td>830</td>\n","      <td>1.763900</td>\n","    </tr>\n","    <tr>\n","      <td>840</td>\n","      <td>1.678700</td>\n","    </tr>\n","    <tr>\n","      <td>850</td>\n","      <td>1.609100</td>\n","    </tr>\n","    <tr>\n","      <td>860</td>\n","      <td>1.610000</td>\n","    </tr>\n","    <tr>\n","      <td>870</td>\n","      <td>1.567100</td>\n","    </tr>\n","    <tr>\n","      <td>880</td>\n","      <td>1.618100</td>\n","    </tr>\n","    <tr>\n","      <td>890</td>\n","      <td>1.617000</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>1.530500</td>\n","    </tr>\n","    <tr>\n","      <td>910</td>\n","      <td>1.536200</td>\n","    </tr>\n","    <tr>\n","      <td>920</td>\n","      <td>1.465700</td>\n","    </tr>\n","    <tr>\n","      <td>930</td>\n","      <td>1.519000</td>\n","    </tr>\n","    <tr>\n","      <td>940</td>\n","      <td>1.526900</td>\n","    </tr>\n","    <tr>\n","      <td>950</td>\n","      <td>1.461200</td>\n","    </tr>\n","    <tr>\n","      <td>960</td>\n","      <td>1.413100</td>\n","    </tr>\n","    <tr>\n","      <td>970</td>\n","      <td>1.430600</td>\n","    </tr>\n","    <tr>\n","      <td>980</td>\n","      <td>1.419700</td>\n","    </tr>\n","    <tr>\n","      <td>990</td>\n","      <td>1.425000</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.473700</td>\n","    </tr>\n","    <tr>\n","      <td>1010</td>\n","      <td>1.455500</td>\n","    </tr>\n","    <tr>\n","      <td>1020</td>\n","      <td>1.354400</td>\n","    </tr>\n","    <tr>\n","      <td>1030</td>\n","      <td>1.430900</td>\n","    </tr>\n","    <tr>\n","      <td>1040</td>\n","      <td>1.414200</td>\n","    </tr>\n","    <tr>\n","      <td>1050</td>\n","      <td>1.408600</td>\n","    </tr>\n","    <tr>\n","      <td>1060</td>\n","      <td>1.424900</td>\n","    </tr>\n","    <tr>\n","      <td>1070</td>\n","      <td>1.428400</td>\n","    </tr>\n","    <tr>\n","      <td>1080</td>\n","      <td>1.300000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=1080, training_loss=1.2679100743046514, metrics={'train_runtime': 13009.7966, 'train_samples_per_second': 2.632, 'train_steps_per_second': 0.083, 'total_flos': 2.7950887435921e+17, 'train_loss': 1.2679100743046514, 'epoch': 18.0})"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["trainer_low_lr.train(resume_from_checkpoint=\"/kaggle/working/qwen-qlora-it-2-v6/checkpoint-600\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T20:47:23.858118Z","iopub.status.busy":"2025-11-29T20:47:23.857388Z","iopub.status.idle":"2025-11-29T20:48:13.741718Z","shell.execute_reply":"2025-11-29T20:48:13.740877Z","shell.execute_reply.started":"2025-11-29T20:47:23.858085Z"},"id":"mA5Dpsk-Xo8Z","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-11-29 20:47:35.834938: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1764449256.017947      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1764449256.067433      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"]},{"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"]},{"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"]},{"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"]},{"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4e5cbe6a31aa4a49b13c5ae027db1dca","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a6dd0cb0275c4d4ea93578435822daab","version_major":2,"version_minor":0},"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e7523bbabb80448bb826e7750c20020a","version_major":2,"version_minor":0},"text/plain":["merges.txt: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e0c25bd54c504f9b8160027164a5e255","version_major":2,"version_minor":0},"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ffa64129d5e042f3a0ec37ece6ada804","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ffefc59f617643fb9137bc5c10ec4019","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ebce565bf4a74715997446d6df919efb","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","from peft import PeftModel\n","\n","base_model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n","adapter_checkpoint = \"/kaggle/input/lora-best\"   # pick your adapter\n","\n","tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n","\n","# Base model (old)\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    base_model_name,\n","    # device_map=\"auto\",\n","    # load_in_8bit=True,\n","    # torch_dtype=\"auto\"\n",")\n","\n","# Fine-tuned model (new)\n","finetuned_model = PeftModel.from_pretrained(base_model, adapter_checkpoint)\n"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T18:01:30.964886Z","iopub.status.busy":"2025-11-29T18:01:30.964668Z","iopub.status.idle":"2025-11-29T18:01:30.973025Z","shell.execute_reply":"2025-11-29T18:01:30.972385Z","shell.execute_reply.started":"2025-11-29T18:01:30.964870Z"},"trusted":true},"outputs":[],"source":["import re\n","import json\n","from typing import Dict\n","\n","def convert_colon_record_to_target(input_str: str,\n","                                   query: str = \"\",\n","                                   aet: int = 30,\n","                                   resolution_summary: str = \"\") -> str:\n","\n","    # Regex finds keys at the start of lines like \"KeyName:\"\n","    key_pattern = re.compile(r'(?m)^(?P<key>[A-Za-z0-9_ ]+):')\n","    matches = list(key_pattern.finditer(input_str))\n","\n","    parsed: Dict[str, str] = {}\n","\n","    # Extract each key's value as substring from end of its colon to start of next key (or end)\n","    for i, m in enumerate(matches):\n","        key_raw = m.group('key').strip()\n","        start = m.end()  # position after the colon\n","        end = matches[i+1].start() if i+1 < len(matches) else len(input_str)\n","        value = input_str[start:end].strip()  # keep internal newlines\n","        # normalize multiple newlines/spaces at edges but preserve paragraphs\n","        value = re.sub(r'\\r\\n', '\\n', value).strip()\n","        parsed[key_raw] = value\n","\n","    # Map parsed keys (case-insensitive) to desired output keys\n","    key_map = {\n","        \"symptoms\": \"Symptoms\",\n","        \"short_description\": \"Short_description\",\n","        \"long_description\": \"Long_description\",\n","        \"causes\": \"Causes\",\n","        \"resolution_note\": \"Resolution_note\"\n","    }\n","\n","    output = {\n","        \"Query\": query,\n","        \"Symptoms\": \"\",\n","        \"Short_description\": \"\",\n","        \"Long_description\": \"\",\n","        \"Causes\": \"\",\n","        \"Resolution_note\": \"\"\n","    }\n","\n","    # Fill output from parsed values where possible\n","    for raw_k, v in parsed.items():\n","        lookup = raw_k.strip().lower().replace(' ', '_')\n","        target_key = key_map.get(lookup)\n","        if target_key:\n","            output[target_key] = v\n","        else:\n","            # If unknown key, add it with a readable name (capitalize, underscores)\n","            fallback_key = raw_k.strip().replace(' ', '_').capitalize()\n","            # avoid clobbering existing keys\n","            if fallback_key in output:\n","                # append with suffix if collision\n","                fallback_key = fallback_key + \"_extra\"\n","            output[fallback_key] = v\n","\n","    # Produce pretty JSON and return inside json markdown block\n","    json_output = json.dumps(output, indent=2, ensure_ascii=False)\n","    return f\"```json\\n{json_output}\\n```\"\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-11-30T12:21:36.977253Z","iopub.status.busy":"2025-11-30T12:21:36.976649Z","iopub.status.idle":"2025-11-30T12:21:36.982316Z","shell.execute_reply":"2025-11-30T12:21:36.981518Z","shell.execute_reply.started":"2025-11-30T12:21:36.977228Z"},"trusted":true},"outputs":[],"source":["\n","\n","\n","def generate(_model, system_prompt, user_query):\n","    user_query = f\"{user_query} \\n\"\n","    messages = [\n","        {\"role\": \"system\", \"content\": system_prompt},\n","        {\"role\": \"user\", \"content\": user_query}\n","    ]\n","    print(messages)\n","    \n","    text = tokenizer.apply_chat_template(\n","        messages,\n","        tokenize=False,\n","        add_generation_prompt=True\n","    )\n","    model_inputs = tokenizer([text], return_tensors=\"pt\").to(_model.device)\n","    \n","    generated_ids = _model.generate(\n","        **model_inputs,\n","        max_new_tokens=512\n","    )\n","    generated_ids = [\n","        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n","    ]\n","    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","    # return parser.invoke(convert_colon_record_to_target(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]))\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-11-30T12:21:26.065930Z","iopub.status.busy":"2025-11-30T12:21:26.065369Z","iopub.status.idle":"2025-11-30T12:21:26.535303Z","shell.execute_reply":"2025-11-30T12:21:26.534529Z","shell.execute_reply.started":"2025-11-30T12:21:26.065905Z"},"trusted":true},"outputs":[],"source":["prompts = [\n","    \"HCL Domino server has been experiencing frequent crashes\",\n","    \"Excessive System Resources Usage by Riverbed SteelCentral NPM\",\n","    \"VLAN communication issue on Cisco Nexus switch\"\n","\n","]\n","import gc\n","import torch\n","\n","# Delete any large objects (models, datasets) you no longer need\n","# del model\n","# del trainer\n","gc.collect()\n","\n","# Clear PyTorch CUDA cache\n","torch.cuda.empty_cache()\n","\n","# Optionally reset the CUDA memory allocator\n","torch.cuda.reset_peak_memory_stats()\n","torch.cuda.reset_accumulated_memory_stats()\n"]},{"cell_type":"code","execution_count":89,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T19:38:19.026870Z","iopub.status.busy":"2025-11-29T19:38:19.026583Z","iopub.status.idle":"2025-11-29T19:42:42.075527Z","shell.execute_reply":"2025-11-29T19:42:42.074724Z","shell.execute_reply.started":"2025-11-29T19:38:19.026851Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n"," ====================================================================================================\n","---------------------------------------- ko ----------------------------------------\n","[{'role': 'system', 'content': 'You are an expert IT assistant for Knowledge Article generation. Analyze the query and produce one complete KO object with detailed symptoms, descriptions, causes, and a 150+ word actionable Resolution_note. Output must strictly follow the JSON structure below exactly as written.\\n```json\\n{\\n  \"Query\": \"...\",\\n  \"Symptoms\": \"...\",\\n  \"Short_description\": \"...\",\\n  \"Long_description\": \"...\",\\n  \"Causes\": \"...\",\\n  \"Resolution_note\": \"..\"\\n}\\n```'}, {'role': 'user', 'content': 'HCL Domino server has been experiencing frequent crashes \\n'}]\n","\n","******************** FINTUNED ********************\n","```json\n","{\n","  \"Symptoms\": \"The server is crashing frequently and unexpectedly, causing disruptions in service availability.\",\n","  \"Short_description\": \"Frequent crashes on HCL Domino server\",\n","  \"Long_description\": \"This article provides information on how to troubleshoot and resolve frequent crashes on an HCL Domino server. It covers the underlying causes of the issue, symptom analysis, and step-by-step resolution instructions.\",\n","  \"Causes\": \"The root causes of frequent crashes on an HCL Domino server can be attributed to various factors, including hardware failures, software conflicts, insufficient resources, network issues, and security vulnerabilities.\",\n","  \"Resolution_note\": \"To resolve frequent crashes on an HCL Domino server, follow these steps:<br><br>1. Check the hardware components of the server, including the CPU, RAM, and hard drive, for any faults or failures.<br>2. Identify any software conflicts or compatibility issues that may be causing the crashes, and uninstall or disable the conflicting software.<br>3. Ensure that the server has sufficient resources, including CPU, RAM, and disk space, and that it is not overloaded or memory-constrained.<br>4. Check the network connectivity and stability of the server, and address any network issues that may be causing crashes.<br>5. Apply any available patches, updates, or security patches to the server software, and ensure that all security settings are properly configured.<br>6. Monitor the server logs and performance metrics to identify any ongoing issues or potential causes of crashes, and take appropriate action as needed.\"\n","}\n","```\n","\n","\n"," ====================================================================================================\n","---------------------------------------- ko ----------------------------------------\n","[{'role': 'system', 'content': 'You are an expert IT assistant for Knowledge Article generation. Analyze the query and produce one complete KO object with detailed symptoms, descriptions, causes, and a 150+ word actionable Resolution_note. Output must strictly follow the JSON structure below exactly as written.\\n```json\\n{\\n  \"Query\": \"...\",\\n  \"Symptoms\": \"...\",\\n  \"Short_description\": \"...\",\\n  \"Long_description\": \"...\",\\n  \"Causes\": \"...\",\\n  \"Resolution_note\": \"..\"\\n}\\n```'}, {'role': 'user', 'content': 'Excessive System Resources Usage by Riverbed SteelCentral NPM \\n'}]\n","\n","******************** FINTUNED ********************\n","```json\n","{\n","  \"Symptoms\": \"High CPU usage, slow system performance, and increased memory usage\",\n","  \"Short_description\": \"Riverbed SteelCentral NPM experiencing excessive resource usage\",\n","  \"Long_description\": \"Riverbed SteelCentral NPM is a network performance monitoring tool that collects and analyzes network traffic to identify performance issues. However, in some cases, the tool may experience excessive resource usage, causing slow system performance and high CPU usage. This can be caused by a variety of factors, including network congestion, misconfigured settings, or outdated software.\",\n","  \"Causes\": \"The underlying reasons for excessive system resources usage by Riverbed SteelCentral NPM can include network congestion, misconfigured settings, outdated software, or other factors that cause the tool to consume more resources than necessary.\",\n","  \"Resolution_note\": \"To resolve this issue, first identify the root cause of the excessive resource usage. This may involve reviewing system logs, checking network traffic, or consulting with a network administrator. Once the root cause has been identified, take steps to address the issue. This may involve optimizing network traffic, adjusting settings to reduce resource consumption, updating software to the latest version, or implementing additional troubleshooting measures. It is also important to monitor system performance over time to ensure that the issue has not reoccurred and to take proactive measures to prevent similar issues from occurring in the future.\"\n","}\n","```\n","\n","\n"," ====================================================================================================\n","---------------------------------------- ko ----------------------------------------\n","[{'role': 'system', 'content': 'You are an expert IT assistant for Knowledge Article generation. Analyze the query and produce one complete KO object with detailed symptoms, descriptions, causes, and a 150+ word actionable Resolution_note. Output must strictly follow the JSON structure below exactly as written.\\n```json\\n{\\n  \"Query\": \"...\",\\n  \"Symptoms\": \"...\",\\n  \"Short_description\": \"...\",\\n  \"Long_description\": \"...\",\\n  \"Causes\": \"...\",\\n  \"Resolution_note\": \"..\"\\n}\\n```'}, {'role': 'user', 'content': 'VLAN communication issue on Cisco Nexus switch \\n'}]\n","\n","******************** FINTUNED ********************\n","```json\n","{\n","  \"Symptoms\": \"Devices on different VLANs cannot communicate with each other, even though they are connected to the same switch.\",\n","  \"Short_description\": \"This article explains how to troubleshoot VLAN communication issues on Cisco Nexus switches.\",\n","  \"Long_description\": \"When trying to communicate between devices on different VLANs on the same switch, it can be frustrating. This article will guide you through the steps to identify and resolve the issue.\",\n","  \"Causes\": \"The main cause of VLAN communication issues on Cisco Nexus switches is misconfiguration or incorrect tagging of VLANs. Other possible causes include network congestion, faulty cables, or hardware failures.\",\n","  \"Resolution_note\": \"To resolve the VLAN communication issue on a Cisco Nexus switch, follow these steps:<br><br>1. Verify that the VLANs are configured correctly on both ends of the network.<br>2. Check the tagging of the VLANs on the devices connected to the switch.<br>3. Use SNMP or command line access to diagnose the problem.<br>4. If the problem persists, try resetting the switch or replacing faulty components.<br><br>If you still cannot resolve the issue after following these steps, contact your network administrator or Cisco support for further assistance.\"\n","}\n","```\n"]}],"source":["results = []\n","for query in prompts:\n","    print(\"\\n\\n\",(\"====\"*25))\n","    for _type in list([\"ko\"]): #\"causecode\", \"fishbone\", \"sip\", \"5_why\", \"ps\"]):\n","        print((\"----\"*10),_type,(\"----\"*10))\n","        reponse = generate(merged_model, system_prompts.get(\"ko\"), query)\n","        print()\n","        print((\"****\"*5),\"FINTUNED\",(\"****\"*5))\n","        print(reponse)\n","        results.append(reponse)\n","        "]},{"cell_type":"code","execution_count":94,"metadata":{"execution":{"iopub.execute_input":"2025-11-28T10:12:41.160013Z","iopub.status.busy":"2025-11-28T10:12:41.159677Z","iopub.status.idle":"2025-11-28T10:12:41.164347Z","shell.execute_reply":"2025-11-28T10:12:41.163462Z","shell.execute_reply.started":"2025-11-28T10:12:41.159989Z"},"trusted":true},"outputs":[],"source":["# reponse = generate(base_model, system_prompts.get(\"ko\"), \"NCR Aloha Restaurant Enterprise Solution is not properly integrating with other software solutions\")\n","# reponse"]},{"cell_type":"code","execution_count":90,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T19:42:56.722611Z","iopub.status.busy":"2025-11-29T19:42:56.721887Z","iopub.status.idle":"2025-11-29T19:42:56.775341Z","shell.execute_reply":"2025-11-29T19:42:56.774647Z","shell.execute_reply.started":"2025-11-29T19:42:56.722585Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[KO(Symptoms='Devices on different VLANs cannot communicate with each other, even though they are connected to the same switch.', Short_description='This article explains how to troubleshoot VLAN communication issues on Cisco Nexus switches.', Long_description='When trying to communicate between devices on different VLANs on the same switch, it can be frustrating. This article will guide you through the steps to identify and resolve the issue.', Causes='The main cause of VLAN communication issues on Cisco Nexus switches is misconfiguration or incorrect tagging of VLANs. Other possible causes include network congestion, faulty cables, or hardware failures.', Resolution_note='To resolve the VLAN communication issue on a Cisco Nexus switch, follow these steps:<br><br>1. Verify that the VLANs are configured correctly on both ends of the network.<br>2. Check the tagging of the VLANs on the devices connected to the switch.<br>3. Use SNMP or command line access to diagnose the problem.<br>4. If the problem persists, try resetting the switch or replacing faulty components.<br><br>If you still cannot resolve the issue after following these steps, contact your network administrator or Cisco support for further assistance.'),\n"," KO(Symptoms='Devices on different VLANs cannot communicate with each other, even though they are connected to the same switch.', Short_description='This article explains how to troubleshoot VLAN communication issues on Cisco Nexus switches.', Long_description='When trying to communicate between devices on different VLANs on the same switch, it can be frustrating. This article will guide you through the steps to identify and resolve the issue.', Causes='The main cause of VLAN communication issues on Cisco Nexus switches is misconfiguration or incorrect tagging of VLANs. Other possible causes include network congestion, faulty cables, or hardware failures.', Resolution_note='To resolve the VLAN communication issue on a Cisco Nexus switch, follow these steps:<br><br>1. Verify that the VLANs are configured correctly on both ends of the network.<br>2. Check the tagging of the VLANs on the devices connected to the switch.<br>3. Use SNMP or command line access to diagnose the problem.<br>4. If the problem persists, try resetting the switch or replacing faulty components.<br><br>If you still cannot resolve the issue after following these steps, contact your network administrator or Cisco support for further assistance.'),\n"," KO(Symptoms='Devices on different VLANs cannot communicate with each other, even though they are connected to the same switch.', Short_description='This article explains how to troubleshoot VLAN communication issues on Cisco Nexus switches.', Long_description='When trying to communicate between devices on different VLANs on the same switch, it can be frustrating. This article will guide you through the steps to identify and resolve the issue.', Causes='The main cause of VLAN communication issues on Cisco Nexus switches is misconfiguration or incorrect tagging of VLANs. Other possible causes include network congestion, faulty cables, or hardware failures.', Resolution_note='To resolve the VLAN communication issue on a Cisco Nexus switch, follow these steps:<br><br>1. Verify that the VLANs are configured correctly on both ends of the network.<br>2. Check the tagging of the VLANs on the devices connected to the switch.<br>3. Use SNMP or command line access to diagnose the problem.<br>4. If the problem persists, try resetting the switch or replacing faulty components.<br><br>If you still cannot resolve the issue after following these steps, contact your network administrator or Cisco support for further assistance.')]"]},"execution_count":90,"metadata":{},"output_type":"execute_result"}],"source":["[parser.invoke(reponse) for respose in results]\n","# system_prompts.get(\"ko\")"]},{"cell_type":"code","execution_count":107,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T19:47:37.515202Z","iopub.status.busy":"2025-11-29T19:47:37.514890Z","iopub.status.idle":"2025-11-29T19:47:38.082532Z","shell.execute_reply":"2025-11-29T19:47:38.081540Z","shell.execute_reply.started":"2025-11-29T19:47:37.515153Z"},"trusted":true},"outputs":[],"source":["# import shutil\n","\n","# # Zip the folder\n","shutil.rmtree(\"/kaggle/working/llama.cpp\")\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T20:49:27.277837Z","iopub.status.busy":"2025-11-29T20:49:27.277460Z","iopub.status.idle":"2025-11-29T20:49:45.172731Z","shell.execute_reply":"2025-11-29T20:49:45.171942Z","shell.execute_reply.started":"2025-11-29T20:49:27.277812Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('merged_model/tokenizer_config.json',\n"," 'merged_model/special_tokens_map.json',\n"," 'merged_model/chat_template.jinja',\n"," 'merged_model/vocab.json',\n"," 'merged_model/merges.txt',\n"," 'merged_model/added_tokens.json',\n"," 'merged_model/tokenizer.json')"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["merged_model = finetuned_model.merge_and_unload()\n","merged_model.save_pretrained(\"merged_model\")\n","tokenizer.save_pretrained(\"merged_model\")"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T18:29:29.567647Z","iopub.status.busy":"2025-11-29T18:29:29.567372Z","iopub.status.idle":"2025-11-29T18:29:43.841769Z","shell.execute_reply":"2025-11-29T18:29:43.840858Z","shell.execute_reply.started":"2025-11-29T18:29:29.567626Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'llama.cpp'...\n","remote: Enumerating objects: 69587, done.\u001b[K\n","remote: Counting objects: 100% (48/48), done.\u001b[K\n","remote: Compressing objects: 100% (30/30), done.\u001b[K\n","remote: Total 69587 (delta 31), reused 18 (delta 18), pack-reused 69539 (from 2)\u001b[K\n","Receiving objects: 100% (69587/69587), 211.38 MiB | 39.06 MiB/s, done.\n","Resolving deltas: 100% (50439/50439), done.\n","python3: can't open file '/kaggle/working/convert.py': [Errno 2] No such file or directory\n"]}],"source":["!git clone https://github.com/ggerganov/llama.cpp\n","!cd llama.cpp\n"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T18:43:45.168350Z","iopub.status.busy":"2025-11-29T18:43:45.167956Z","iopub.status.idle":"2025-11-29T18:43:45.829095Z","shell.execute_reply":"2025-11-29T18:43:45.828216Z","shell.execute_reply.started":"2025-11-29T18:43:45.168326Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('base_model/tokenizer_config.json',\n"," 'base_model/special_tokens_map.json',\n"," 'base_model/chat_template.jinja',\n"," 'base_model/vocab.json',\n"," 'base_model/merges.txt',\n"," 'base_model/added_tokens.json',\n"," 'base_model/tokenizer.json')"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["model.save_pretrained(\"base_model\")\n","tokenizer.save_pretrained(\"base_model\")"]},{"cell_type":"code","execution_count":76,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T19:13:14.351276Z","iopub.status.busy":"2025-11-29T19:13:14.350989Z","iopub.status.idle":"2025-11-29T19:13:57.397783Z","shell.execute_reply":"2025-11-29T19:13:57.396970Z","shell.execute_reply.started":"2025-11-29T19:13:14.351254Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO:hf-to-gguf:Loading model: merged_model\n","INFO:hf-to-gguf:Model architecture: Qwen2ForCausalLM\n","INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n","INFO:hf-to-gguf:gguf: indexing model part 'model-00002-of-00002.safetensors'\n","INFO:hf-to-gguf:gguf: indexing model part 'model-00001-of-00002.safetensors'\n","INFO:hf-to-gguf:gguf: indexing model part 'model.safetensors'\n","INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n","INFO:hf-to-gguf:Exporting model...\n","INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.22.attn_output.weight, torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.23.attn_output.weight, torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.24.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.24.attn_output.weight, torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.24.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.24.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.25.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.25.attn_output.weight, torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.25.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.25.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.26.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.26.attn_output.weight, torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.26.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.26.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.27.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.27.attn_output.weight, torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.27.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.27.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> Q8_0, shape = {1536, 151936}\n","INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.10.attn_output.weight, torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.11.attn_output.weight, torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.12.attn_output.weight, torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.13.attn_output.weight, torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.14.attn_output.weight, torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.15.attn_output.weight, torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.16.attn_output.weight, torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.17.attn_output.weight, torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.18.attn_output.weight, torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.19.attn_output.weight, torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.20.attn_output.weight, torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.21.attn_output.weight, torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.int8 --> Q8_0, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.int8 --> Q8_0, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.int8 --> Q8_0, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.int8 --> Q8_0, shape = {1536, 256}\n","INFO:hf-to-gguf:Set meta model\n","INFO:hf-to-gguf:Set model parameters\n","INFO:hf-to-gguf:gguf: context length = 32768\n","INFO:hf-to-gguf:gguf: embedding length = 1536\n","INFO:hf-to-gguf:gguf: feed forward length = 8960\n","INFO:hf-to-gguf:gguf: head count = 12\n","INFO:hf-to-gguf:gguf: key-value head count = 2\n","INFO:hf-to-gguf:gguf: rope theta = 1000000.0\n","INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n","INFO:hf-to-gguf:gguf: file type = 7\n","INFO:hf-to-gguf:Set model quantization version\n","INFO:hf-to-gguf:Set model tokenizer\n","INFO:gguf.vocab:Adding 151387 merge(s).\n","INFO:gguf.vocab:Setting special token type eos to 151645\n","INFO:gguf.vocab:Setting special token type pad to 151643\n","INFO:gguf.vocab:Setting special token type bos to 151643\n","INFO:gguf.vocab:Setting add_bos_token to False\n","INFO:gguf.vocab:Setting chat_template to {%- if tools %}\n","    {{- '<|im_start|>system\\n' }}\n","    {%- if messages[0]['role'] == 'system' %}\n","        {{- messages[0]['content'] }}\n","    {%- else %}\n","        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n","    {%- endif %}\n","    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n","    {%- for tool in tools %}\n","        {{- \"\\n\" }}\n","        {{- tool | tojson }}\n","    {%- endfor %}\n","    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n","{%- else %}\n","    {%- if messages[0]['role'] == 'system' %}\n","        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n","    {%- else %}\n","        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n","    {%- endif %}\n","{%- endif %}\n","{%- for message in messages %}\n","    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n","        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n","    {%- elif message.role == \"assistant\" %}\n","        {{- '<|im_start|>' + message.role }}\n","        {%- if message.content %}\n","            {{- '\\n' + message.content }}\n","        {%- endif %}\n","        {%- for tool_call in message.tool_calls %}\n","            {%- if tool_call.function is defined %}\n","                {%- set tool_call = tool_call.function %}\n","            {%- endif %}\n","            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n","            {{- tool_call.name }}\n","            {{- '\", \"arguments\": ' }}\n","            {{- tool_call.arguments | tojson }}\n","            {{- '}\\n</tool_call>' }}\n","        {%- endfor %}\n","        {{- '<|im_end|>\\n' }}\n","    {%- elif message.role == \"tool\" %}\n","        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n","            {{- '<|im_start|>user' }}\n","        {%- endif %}\n","        {{- '\\n<tool_response>\\n' }}\n","        {{- message.content }}\n","        {{- '\\n</tool_response>' }}\n","        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n","            {{- '<|im_end|>\\n' }}\n","        {%- endif %}\n","    {%- endif %}\n","{%- endfor %}\n","{%- if add_generation_prompt %}\n","    {{- '<|im_start|>assistant\\n' }}\n","{%- endif %}\n","\n","INFO:gguf.gguf_writer:Writing the following files:\n","INFO:gguf.gguf_writer:/kaggle/working/my_model.Q8_0.gguf: n_tensors = 338, total_size = 1.6G\n","Writing: 100%|██████████████████████████| 1.64G/1.64G [00:34<00:00, 47.1Mbyte/s]\n","INFO:hf-to-gguf:Model successfully exported to /kaggle/working/my_model.Q8_0.gguf\n"]}],"source":["!python3 convert_hf_to_gguf.py \\\n","  /kaggle/working/llama.cpp/merged_model \\\n","  --outfile /kaggle/working/my_model.Q8_0.gguf \\\n","  --outtype q8_0"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T18:43:24.411546Z","iopub.status.busy":"2025-11-29T18:43:24.411274Z","iopub.status.idle":"2025-11-29T18:43:24.521859Z","shell.execute_reply":"2025-11-29T18:43:24.521291Z","shell.execute_reply.started":"2025-11-29T18:43:24.411526Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Deleted: /kaggle/working/merged_model\n"]}],"source":["import shutil\n","import os\n","\n","# Example: delete a folder named \"llama.cpp\"\n","folder_path = \"/kaggle/working/merged_model\"\n","\n","if os.path.exists(folder_path):\n","    shutil.rmtree(folder_path)\n","    print(f\"Deleted: {folder_path}\")\n","else:\n","    print(\"Folder not found\")\n"]},{"cell_type":"code","execution_count":77,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T19:18:21.866624Z","iopub.status.busy":"2025-11-29T19:18:21.865987Z","iopub.status.idle":"2025-11-29T19:18:22.465267Z","shell.execute_reply":"2025-11-29T19:18:22.464515Z","shell.execute_reply.started":"2025-11-29T19:18:21.866590Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["zip I/O error: No such file or directory\n","zip error: Could not create output file (/kaggle/outputs/my_model.zip)\n"]}],"source":["!zip -j /kaggle/outputs/my_model.zip /kaggle/working/my_model.Q8_0.gguf\n"]},{"cell_type":"code","execution_count":79,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T19:20:03.010194Z","iopub.status.busy":"2025-11-29T19:20:03.009892Z","iopub.status.idle":"2025-11-29T19:20:04.268609Z","shell.execute_reply":"2025-11-29T19:20:04.267991Z","shell.execute_reply.started":"2025-11-29T19:20:03.010149Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'/kaggle/outputs/my_model.Q8_0.gguf'"]},"execution_count":79,"metadata":{},"output_type":"execute_result"}],"source":["import os, shutil\n","\n","# Make sure the outputs directory exists\n","os.makedirs(\"/kaggle/outputs\", exist_ok=True)\n","\n","# Now copy your file\n","shutil.copy(\"/kaggle/working/my_model.Q8_0.gguf\", \"/kaggle/outputs/my_model.Q8_0.gguf\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## hugging face"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T20:50:41.658812Z","iopub.status.busy":"2025-11-29T20:50:41.657804Z","iopub.status.idle":"2025-11-29T20:50:41.722512Z","shell.execute_reply":"2025-11-29T20:50:41.721944Z","shell.execute_reply.started":"2025-11-29T20:50:41.658785Z"},"trusted":true},"outputs":[],"source":["\n","from huggingface_hub import login\n","\n","# Paste your token here (keep it private!)\n","login(\"hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T20:55:36.977172Z","iopub.status.busy":"2025-11-29T20:55:36.976283Z","iopub.status.idle":"2025-11-29T20:56:24.639196Z","shell.execute_reply":"2025-11-29T20:56:24.638545Z","shell.execute_reply.started":"2025-11-29T20:55:36.977134Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"33cd3167cceb4dc2aa3b77c8ff1f8404","version_major":2,"version_minor":0},"text/plain":["Processing Files (0 / 0): |          |  0.00B /  0.00B            "]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"81564ef6fe2c4e30aec12b282dc0460c","version_major":2,"version_minor":0},"text/plain":["New Data Upload: |          |  0.00B /  0.00B            "]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/siddharth797/Qwen2.5-1.5b-full/commit/829951231d492ec90d22e008b4a54434e1833cee', commit_message='Upload merged fp16 model', commit_description='', oid='829951231d492ec90d22e008b4a54434e1833cee', pr_url=None, repo_url=RepoUrl('https://huggingface.co/siddharth797/Qwen2.5-1.5b-full', endpoint='https://huggingface.co', repo_type='model', repo_id='siddharth797/Qwen2.5-1.5b-full'), pr_revision=None, pr_num=None)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["from huggingface_hub import upload_folder\n","\n","# Path to your merged model\n","model_path = \"/kaggle/working/merged_model\"\n","\n","# Your repo ID on Hugging Face (username/repo_name)\n","repo_id = \"siddharth797/Qwen2.5-1.5b-full\"\n","\n","upload_folder(\n","    repo_id=repo_id,\n","    folder_path=model_path,\n","    commit_message=\"Upload merged fp16 model\"\n",")\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-11-29T20:50:47.843120Z","iopub.status.busy":"2025-11-29T20:50:47.842379Z","iopub.status.idle":"2025-11-29T20:50:49.291449Z","shell.execute_reply":"2025-11-29T20:50:49.290749Z","shell.execute_reply.started":"2025-11-29T20:50:47.843095Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33m⚠️  Warning: 'huggingface-cli repo' is deprecated. Use 'hf repo' instead.\u001b[0m\n","Successfully created \u001b[1msiddharth797/Qwen2.5-1.5b-full\u001b[0m on the Hub.\n","Your repo is now available at \u001b[1mhttps://huggingface.co/siddharth797/Qwen2.5-1.5b-full\u001b[0m\n"]}],"source":["!huggingface-cli repo create Qwen2.5-1.5b-full"]},{"cell_type":"markdown","metadata":{},"source":["## ONNX"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!optimum-cli export onnx --model ./Qwen2.5-1.5b --task causal-lm ./onnx_qwen2"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-11-30T11:55:00.821852Z","iopub.status.busy":"2025-11-30T11:55:00.821600Z","iopub.status.idle":"2025-11-30T11:56:21.120650Z","shell.execute_reply":"2025-11-30T11:56:21.119427Z","shell.execute_reply.started":"2025-11-30T11:55:00.821830Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-11-30 11:55:14.777797: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1764503714.979005      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1764503715.039361      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"]},{"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"]},{"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"]},{"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"]},{"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b6db1016a5c545e68438e6c17c7097f9","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"62a1b9073090447db5fb6abec11018a1","version_major":2,"version_minor":0},"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7543186d08524fb9ae539ba3b6fded38","version_major":2,"version_minor":0},"text/plain":["merges.txt: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f06cf9a880344890b7cbd792a62940e9","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fa040707e4b74e15bb081641dc908371","version_major":2,"version_minor":0},"text/plain":["added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"afd0153f3d9843b49f0dd82206d7f9c6","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"34ab0461f33740b08c248c0737c33b17","version_major":2,"version_minor":0},"text/plain":["chat_template.jinja: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"854c2df1894646e7bf2a45b3249cefb6","version_major":2,"version_minor":0},"text/plain":["config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6d50042e58904a3e9d942be7e846060d","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a95d28cc2f974df29dccdbb1cdcdedae","version_major":2,"version_minor":0},"text/plain":["Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6f61cec33cf84672b5bf2be871a2e57c","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b2cc75a4461e436bb21ca80ac3da1ae0","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"19b86243ae7b4bf9b8e22a31d65bccdb","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"122950d979724defac557231445f0239","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["('Qwen2.5-1.5b-ikon-full/tokenizer_config.json',\n"," 'Qwen2.5-1.5b-ikon-full/special_tokens_map.json',\n"," 'Qwen2.5-1.5b-ikon-full/chat_template.jinja',\n"," 'Qwen2.5-1.5b-ikon-full/vocab.json',\n"," 'Qwen2.5-1.5b-ikon-full/merges.txt',\n"," 'Qwen2.5-1.5b-ikon-full/added_tokens.json',\n"," 'Qwen2.5-1.5b-ikon-full/tokenizer.json')"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","from peft import PeftModel\n","\n","# base_model_name = \"siddharth797/Qwen2.5-1.5b\"\n","base_model_name = \"siddharth797/Qwen2.5-1.5b-full\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    base_model_name,\n","    # load_in_4bit=True,\n","    # dtype=\"int4\", \n","    torch_dtype=\"auto\",\n",")\n","\n","model.save_pretrained(\"Qwen2.5-1.5b-ikon-full\")\n","tokenizer.save_pretrained(\"Qwen2.5-1.5b-ikon-full\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-11-30T12:13:28.988429Z","iopub.status.busy":"2025-11-30T12:13:28.988197Z","iopub.status.idle":"2025-11-30T12:18:01.156383Z","shell.execute_reply":"2025-11-30T12:18:01.155506Z","shell.execute_reply.started":"2025-11-30T12:13:28.988404Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Multiple distributions found for package optimum. Picked distribution: optimum-onnx\n","2025-11-30 12:13:46.257884: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1764504826.409575      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1764504826.455615      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"]},{"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"]},{"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"]},{"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"]},{"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9fd77eb2218643e48883611a01bbd7f3","version_major":2,"version_minor":0},"text/plain":["config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8e5f28aa53e84d80b1e8df624b8f0494","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"343493b63ae74e389fedb30c725a128d","version_major":2,"version_minor":0},"text/plain":["Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"23ef1baa32654b5d87e2be8689d6900f","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bdb99872bd8c45dcb194c594895f9ba7","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"034e7cc84d264043a17f7f3943d81b42","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f1e0d838e1b74a31beb9e5268c949b51","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"36e5ab614de9452ab55c5b9d6d593a1c","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8d9f4c2a8f2649febf56a3e5b65c66a8","version_major":2,"version_minor":0},"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce84ba6bbbcc407d86383e44a056878b","version_major":2,"version_minor":0},"text/plain":["merges.txt: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fb96d8124e5542319dcf5798cb58da2e","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca005e16df3448ba97e937289de8ce6c","version_major":2,"version_minor":0},"text/plain":["added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"675ea4c5037541aa82d074e12676c4f2","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"661734656b3443708cf427c5e0121cf0","version_major":2,"version_minor":0},"text/plain":["chat_template.jinja: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/transformers/cache_utils.py:568: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  or not self.key_cache[layer_idx].numel()  # the layer has no cache\n","/usr/local/lib/python3.11/dist-packages/transformers/masking_utils.py:187: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if (padding_length := kv_length + kv_offset - attention_mask.shape[-1]) > 0:\n","/usr/local/lib/python3.11/dist-packages/transformers/masking_utils.py:215: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if padding_mask is not None and padding_mask.shape[-1] > kv_length:\n","/usr/local/lib/python3.11/dist-packages/transformers/cache_utils.py:551: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  elif (\n","/usr/local/lib/python3.11/dist-packages/transformers/integrations/sdpa_attention.py:59: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, \"is_causal\", True)\n","/usr/local/lib/python3.11/dist-packages/torch/onnx/symbolic_opset9.py:5383: UserWarning: Exporting aten::index operator of advanced indexing in opset 18 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n","  warnings.warn(\n","Found different candidate ONNX initializers (likely duplicate) for the tied weights:\n","\tlm_head.weight: {'onnx::MatMul_9340'}\n","\tmodel.embed_tokens.weight: {'model.embed_tokens.weight'}\n"]},{"data":{"text/plain":["('./onnx_qwen2/tokenizer_config.json',\n"," './onnx_qwen2/special_tokens_map.json',\n"," './onnx_qwen2/chat_template.jinja',\n"," './onnx_qwen2/vocab.json',\n"," './onnx_qwen2/merges.txt',\n"," './onnx_qwen2/added_tokens.json',\n"," './onnx_qwen2/tokenizer.json')"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["from optimum.onnxruntime import ORTModelForCausalLM\n","from transformers import AutoTokenizer\n","\n","model_id = \"siddharth797/Qwen2.5-1.5b-full\"\n","\n","# Export to ONNX with cache enabled\n","ort_model = ORTModelForCausalLM.from_pretrained(\n","    model_id,\n","    export=True,\n","    use_cache=True\n",")\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","ort_model.save_pretrained(\"./onnx_qwen2\")\n","tokenizer.save_pretrained(\"./onnx_qwen2\")\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-11-30T12:24:01.075992Z","iopub.status.busy":"2025-11-30T12:24:01.075666Z","iopub.status.idle":"2025-11-30T12:28:49.931560Z","shell.execute_reply":"2025-11-30T12:28:49.930641Z","shell.execute_reply.started":"2025-11-30T12:24:01.075964Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n"," ====================================================================================================\n","---------------------------------------- ko ----------------------------------------\n","[{'role': 'system', 'content': 'You are an expert IT assistant for Knowledge Article generation. Analyze the query and produce one complete KO object with detailed symptoms, descriptions, causes, and a 150+ word actionable Resolution_note. Output must strictly follow the JSON structure below exactly as written.\\n```json\\n{\\n  \"Query\": \"...\",\\n  \"Symptoms\": \"...\",\\n  \"Short_description\": \"...\",\\n  \"Long_description\": \"...\",\\n  \"Causes\": \"...\",\\n  \"Resolution_note\": \"..\"\\n}\\n```'}, {'role': 'user', 'content': 'HCL Domino server has been experiencing frequent crashes \\n'}]\n","\n","******************** FINTUNED ********************\n","```json\n","{\n","  \"Symptoms\": \"The server is crashing frequently and unexpectedly, causing disruptions in service availability.\",\n","  \"Short_description\": \"Frequent crashes on HCL Domino server\",\n","  \"Long_description\": \"This article provides information on how to troubleshoot and resolve frequent crashes on an HCL Domino server. It covers the underlying causes of the issue, symptoms to look out for, and step-by-step instructions for resolving the problem.\",\n","  \"Causes\": \"The underlying causes of frequent crashes on an HCL Domino server can include hardware issues, software conflicts, insufficient resources, and network connectivity problems.\",\n","  \"Resolution_note\": \"To resolve frequent crashes on an HCL Domino server, follow these steps:<br><br>1. Check the hardware components of the server, including the CPU, RAM, and hard drive, to ensure they are functioning properly.<br>2. Identify any software conflicts or compatibility issues that may be causing the crashes, and uninstall or disable any conflicting software.<br>3. Ensure that the server has sufficient resources, including CPU, RAM, and disk space, and that it is not overloaded or running too many processes at once.<br>4. Check the network connectivity of the server, and ensure that it is properly configured and secured.<br>5. If the issue persists, contact HCL support for further assistance.\"\n","}\n","```\n","\n","\n"," ====================================================================================================\n","---------------------------------------- ko ----------------------------------------\n","[{'role': 'system', 'content': 'You are an expert IT assistant for Knowledge Article generation. Analyze the query and produce one complete KO object with detailed symptoms, descriptions, causes, and a 150+ word actionable Resolution_note. Output must strictly follow the JSON structure below exactly as written.\\n```json\\n{\\n  \"Query\": \"...\",\\n  \"Symptoms\": \"...\",\\n  \"Short_description\": \"...\",\\n  \"Long_description\": \"...\",\\n  \"Causes\": \"...\",\\n  \"Resolution_note\": \"..\"\\n}\\n```'}, {'role': 'user', 'content': 'Excessive System Resources Usage by Riverbed SteelCentral NPM \\n'}]\n","\n","******************** FINTUNED ********************\n","```json\n","{\n","  \"Symptoms\": \"Slow network performance, high CPU usage, and slow disk I/O\",\n","  \"Short_description\": \"Riverbed SteelCentral NPM experiencing excessive system resource usage\",\n","  \"Long_description\": \"Riverbed SteelCentral NPM is a network performance monitoring tool that provides real-time visibility into network traffic. However, in some cases, the tool may experience excessive system resource usage, leading to slow network performance, high CPU usage, and slow disk I/O. This can be caused by a variety of factors, including misconfigured settings, outdated software, or insufficient system resources.\",\n","  \"Causes\": \"The underlying causes of excessive system resource usage by Riverbed SteelCentral NPM can include misconfigured settings, outdated software, insufficient system resources, or conflicts with other applications running on the same system.\",\n","  \"Resolution_note\": \"To resolve this issue, follow these steps:<br><br>1. Check the system resources: Use the Task Manager or similar utility to check the CPU, memory, and disk I/O usage on the system. If any of these values are high, it may indicate a resource contention issue.<br><br>2. Check the NPM configuration: Review the NPM configuration settings to ensure that they are optimized for your environment. Look for settings that may be misconfigured or outdated, such as the number of threads or the buffer size.<br><br>3. Update the NPM software: If the NPM software is outdated, consider updating it to the latest version. The latest versions often include bug fixes and performance improvements.<br><br>4. Add more system resources: If the system resources are insufficient, consider adding more RAM or CPU to the system. This may require upgrading the hardware or using virtualization technology to allocate additional resources.<br><br>5. Check for conflicts: If there are any conflicting applications running on the same system, try disabling them or adjusting their settings to reduce their resource usage.<br><br>By following these steps, you should be able to resolve the issue of excessive system resource usage by Riverbed SteelCentral NPM.\"\n","}\n","```\n","\n","\n"," ====================================================================================================\n","---------------------------------------- ko ----------------------------------------\n","[{'role': 'system', 'content': 'You are an expert IT assistant for Knowledge Article generation. Analyze the query and produce one complete KO object with detailed symptoms, descriptions, causes, and a 150+ word actionable Resolution_note. Output must strictly follow the JSON structure below exactly as written.\\n```json\\n{\\n  \"Query\": \"...\",\\n  \"Symptoms\": \"...\",\\n  \"Short_description\": \"...\",\\n  \"Long_description\": \"...\",\\n  \"Causes\": \"...\",\\n  \"Resolution_note\": \"..\"\\n}\\n```'}, {'role': 'user', 'content': 'VLAN communication issue on Cisco Nexus switch \\n'}]\n","\n","******************** FINTUNED ********************\n","```json\n","{\n","  \"Symptoms\": \"Devices on different VLANs cannot communicate with each other, even though their IPs are in the same subnet.\",\n","  \"Short_description\": \"This article explains how to troubleshoot VLAN communication issues on Cisco Nexus switches.\",\n","  \"Long_description\": \"If you are experiencing VLAN communication issues on your Cisco Nexus switch, it can be frustrating and time-consuming to resolve. This article will guide you through the steps to diagnose and fix the problem. We will cover common causes of VLAN communication issues, symptoms to look out for, and step-by-step instructions to resolve the issue.\",\n","  \"Causes\": \"There are several underlying reasons that can cause VLAN communication issues on Cisco Nexus switches. Some common causes include incorrect configuration, misconfigured ports, faulty hardware, or network congestion.\",\n","  \"Resolution_note\": \"To resolve VLAN communication issues on your Cisco Nexus switch, follow these steps:<br><br>1. Verify that the VLANs are configured correctly on both ends of the network.<br>2. Check the physical connectivity of the switches and cables.<br>3. Verify that the ports are not crossed or configured in the wrong manner.<br>4. Check for any network congestion or bandwidth limitations.<br>5. If the issue persists, try resetting the switch to its default settings and reconfiguring the network.<br><br>If none of these steps resolve the issue, contact your network administrator or Cisco support for further assistance.\"\n","}\n","```\n"]}],"source":["for query in prompts:\n","    print(\"\\n\\n\",(\"====\"*25))\n","    for _type in [\"ko\"]:#, \"causecode\", \"fishbone\", \"sip\", \"5_why\", \"ps\"]:\n","        print((\"----\"*10),_type,(\"----\"*10))\n","        reponse = generate(ort_model, system_prompts.get(\"ko\"), query)\n","        print()\n","        print((\"****\"*5),\"FINTUNED\",(\"****\"*5))\n","        print(reponse)\n","        "]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-11-30T12:29:05.255517Z","iopub.status.busy":"2025-11-30T12:29:05.254769Z","iopub.status.idle":"2025-11-30T12:30:36.097189Z","shell.execute_reply":"2025-11-30T12:30:36.096554Z","shell.execute_reply.started":"2025-11-30T12:29:05.255489Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"]},{"name":"stdout","output_type":"stream","text":["✅ Quantized model saved at: /kaggle/working/onnx_qwen2/model_int8.onnx\n"]}],"source":["from onnxruntime.quantization import quantize_dynamic, QuantType\n","\n","# Paths\n","onnx_model_path = \"/kaggle/working/onnx_qwen2/model.onnx\"\n","quantized_model_path = \"/kaggle/working/onnx_qwen2/model_int8.onnx\"\n","\n","# Quantize weights to INT8\n","quantize_dynamic(\n","    model_input=onnx_model_path,\n","    model_output=quantized_model_path,\n","    weight_type=QuantType.QInt8\n",")\n","\n","print(\"✅ Quantized model saved at:\", quantized_model_path)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2025-11-30T12:35:39.233873Z","iopub.status.busy":"2025-11-30T12:35:39.233302Z","iopub.status.idle":"2025-11-30T12:39:35.954380Z","shell.execute_reply":"2025-11-30T12:39:35.953334Z","shell.execute_reply.started":"2025-11-30T12:35:39.233845Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'role': 'system', 'content': 'You are an expert IT assistant for Knowledge Article generation. Analyze the query and produce one complete KO object with detailed symptoms, descriptions, causes, and a 150+ word actionable Resolution_note. Output must strictly follow the JSON structure below exactly as written.\\n```json\\n{\\n  \"Query\": \"...\",\\n  \"Symptoms\": \"...\",\\n  \"Short_description\": \"...\",\\n  \"Long_description\": \"...\",\\n  \"Causes\": \"...\",\\n  \"Resolution_note\": \"..\"\\n}\\n```'}, {'role': 'user', 'content': 'HCL Domino server has been experiencing frequent crashes \\n'}]\n","\n","FP32 ONNX:\n","```json\n","{\n","  \"Symptoms\": \"The server is crashing frequently and unexpectedly, causing disruptions in service availability.\",\n","  \"Short_description\": \"Frequent crashes on HCL Domino server\",\n","  \"Long_description\": \"This article provides information on how to troubleshoot and resolve frequent crashes on an HCL Domino server. It covers the symptoms, causes, and step-by-step resolution process.\",\n","  \"Causes\": \"The underlying reasons for frequent crashes on an HCL Domino server can be due to various factors such as hardware issues, software conflicts, outdated drivers, or insufficient resources.\",\n","  \"Resolution_note\": \"To resolve frequent crashes on an HCL Domino server, follow these steps:<br><br>1. Check the hardware components of the server, including the CPU, RAM, and hard drive usage, to ensure that they are not overloaded or faulty.<br>2. Verify that all software applications and drivers are up-to-date and compatible with the HCL Domino server.<br>3. Monitor the server logs for any error messages or warnings that may indicate the root cause of the crashes.<br>4. Restart the HCL Domino server and test its performance to determine if the crashes have ceased.<br>5. If the issue persists, contact technical support for further assistance.\"\n","}\n","```\n","Generated tokens in 76.54 sec\n","[{'role': 'system', 'content': 'You are an expert IT assistant for Knowledge Article generation. Analyze the query and produce one complete KO object with detailed symptoms, descriptions, causes, and a 150+ word actionable Resolution_note. Output must strictly follow the JSON structure below exactly as written.\\n```json\\n{\\n  \"Query\": \"...\",\\n  \"Symptoms\": \"...\",\\n  \"Short_description\": \"...\",\\n  \"Long_description\": \"...\",\\n  \"Causes\": \"...\",\\n  \"Resolution_note\": \"..\"\\n}\\n```'}, {'role': 'user', 'content': 'HCL Domino server has been experiencing frequent crashes \\n'}]\n","\n","INT8 ONNX:\n","There have been several reports of HCL Domino crashing frequently recently. This can be caused by a number of factors, including outdated software, hardware issues, or network connectivity problems. To resolve this issue, it is recommended to perform a system scan to identify any conflicts or compatibility issues with the software. Additionally, updating to the latest version of HCL Domino and ensuring that all hardware and network connections are stable can help prevent future crashes. It is also important to regularly backup data and to have a disaster recovery plan in place to minimize any potential downtime. <br><br>As for the symptom you mentioned, it would be helpful if you could provide more specific details about the nature of the crash and any error messages that may appear. This information can then be used to further troubleshoot and diagnose the issue. Overall, it is important to handle HCL Domino crashes promptly and thoroughly to ensure the stability and reliability of your enterprise collaboration platform. <br><br>Note: If the issue persists, it is recommended to contact HCL support for further assistance. <br><br>KO object:<br>Caused by: Outdated software or hardware issues<br>Description: Frequent crashes when using HCL Domino<br>Actionable Resolution Note: Perform a system scan to identify conflicts and compatibility issues. Update to the latest version of HCL Domino and ensure all hardware and network connections are stable. Regularly backup data and have a disaster recovery plan in place. Contact HCL support for further assistance if the issue persists. <br><br>Short_description: HCL Domino crashes frequently <br>Long_description: HCL Domino is a collaboration platform that experiences frequent crashes, which can cause disruption and loss of productivity for users. It is important to address this issue promptly to ensure the stability and reliability of the platform. <br>\n","```json\n","{\n","  \"Symptoms\": \"The application crashes unexpectedly and frequently, causing loss of work and frustration.\",\n","  \"Causes\": \"The underlying reasons for frequent crashes in the application could be due to outdated software, hardware issues, or compatibility problems.\",\n","  \"Short_description\": \"Frequent crashes in the application\",\n","  \"Long_description\": \"This article provides troubleshooting steps and actionable resolution notes to fix frequent crashes in the application. It covers various scenarios such as outdated software, hardware issues, and compatibility problems. The article also includes tips on how to prevent crashes in the future and what to do in case of an emergency.\",\n","  \"Detailed_symptoms\": \"The application crashes while performing certain tasks, such as saving files\n","Generated tokens in 130.91 sec\n"]}],"source":["import time\n","from transformers import AutoTokenizer\n","from optimum.onnxruntime import ORTModelForCausalLM\n","\n","# Paths\n","base_dir = \"/kaggle/working/onnx_qwen2\"\n","int8_model_path = f\"{base_dir}/model_int8.onnx\"\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(base_dir)\n","\n","# Load baseline ONNX model\n","ort_model_fp32 = ORTModelForCausalLM.from_pretrained(base_dir, file_name=\"model.onnx\")\n","\n","# Load INT8 ONNX model\n","ort_model_int8 = ORTModelForCausalLM.from_pretrained(base_dir, file_name=\"model_int8.onnx\")\n","\n","# Test prompt\n","query = prompts[0]\n","\n","def benchmark(model, name):\n","    inputs = tokenizer(query, return_tensors=\"pt\")\n","    # Warmup\n","    _ = model.generate(**inputs, max_new_tokens=10, use_cache=False)\n","\n","    # Benchmark\n","    start = time.time()\n","    outputs = generate(model, system_prompts.get(\"ko\"), query)\n","    end = time.time()\n","\n","    elapsed = end - start\n","\n","    print(f\"\\n{name}:\")\n","    print(outputs)\n","    print(f\"Generated tokens in {elapsed:.2f} sec\")\n","\n","# Run both\n","benchmark(ort_model_fp32, \"FP32 ONNX\")\n","benchmark(ort_model_int8, \"INT8 ONNX\")\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-11-30T12:12:11.023507Z","iopub.status.busy":"2025-11-30T12:12:11.023250Z","iopub.status.idle":"2025-11-30T12:13:28.986794Z","shell.execute_reply":"2025-11-30T12:13:28.985865Z","shell.execute_reply.started":"2025-11-30T12:12:11.023486Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Skipping optimum as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Skipping optimum-onnxruntime as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Skipping onnxruntime-gpu as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Skipping onnxruntime as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mCollecting optimum\n","  Downloading optimum-2.0.0-py3-none-any.whl.metadata (14 kB)\n","Collecting onnxruntime\n","  Downloading onnxruntime-1.23.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n","Requirement already satisfied: transformers>=4.29 in /usr/local/lib/python3.11/dist-packages (from optimum) (4.53.3)\n","Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.11/dist-packages (from optimum) (2.6.0+cu124)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from optimum) (25.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optimum) (1.26.4)\n","Requirement already satisfied: huggingface_hub>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from optimum) (0.36.0)\n","Collecting coloredlogs (from onnxruntime)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (6.33.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum) (3.20.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum) (2025.10.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum) (6.0.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum) (2.32.5)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum) (1.2.0)\n","Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->optimum) (1.3.8)\n","Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->optimum) (1.2.4)\n","Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->optimum) (0.1.1)\n","Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->optimum) (2025.3.0)\n","Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->optimum) (2022.3.0)\n","Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->optimum) (2.4.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11->optimum)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11->optimum)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11->optimum)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11->optimum)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11->optimum)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11->optimum)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11->optimum)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11->optimum)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11->optimum)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11->optimum)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (3.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum) (2025.11.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum) (0.21.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum) (0.5.3)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11->optimum) (3.0.3)\n","Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optimum) (2025.3.0)\n","Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optimum) (2024.2.0)\n","Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optimum) (2022.3.0)\n","Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->optimum) (1.4.0)\n","Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->optimum) (2024.2.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.8.0->optimum) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.8.0->optimum) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.8.0->optimum) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.8.0->optimum) (2025.10.5)\n","Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->optimum) (2024.2.0)\n","Downloading optimum-2.0.0-py3-none-any.whl (162 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.3/162.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnxruntime-1.23.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, humanfriendly, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, nvidia-cusolver-cu12, optimum, onnxruntime\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n","pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n","pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnxruntime-1.23.2 optimum-2.0.0\n","Requirement already satisfied: optimum[onnxruntime] in /usr/local/lib/python3.11/dist-packages (2.0.0)\n","Requirement already satisfied: transformers>=4.29 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (4.53.3)\n","Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (2.6.0+cu124)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (25.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (1.26.4)\n","Requirement already satisfied: huggingface_hub>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (0.36.0)\n","Collecting optimum-onnx[onnxruntime] (from optimum[onnxruntime])\n","  Downloading optimum_onnx-0.0.3-py3-none-any.whl.metadata (4.6 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (3.20.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (2025.10.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (6.0.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (2.32.5)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (1.2.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11->optimum[onnxruntime]) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum[onnxruntime]) (2025.11.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum[onnxruntime]) (0.21.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum[onnxruntime]) (0.5.3)\n","Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->optimum[onnxruntime]) (1.3.8)\n","Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->optimum[onnxruntime]) (1.2.4)\n","Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->optimum[onnxruntime]) (0.1.1)\n","Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->optimum[onnxruntime]) (2025.3.0)\n","Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->optimum[onnxruntime]) (2022.3.0)\n","Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->optimum[onnxruntime]) (2.4.1)\n","Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (from optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (1.18.0)\n","Requirement already satisfied: onnxruntime>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (1.23.2)\n","Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.18.0->optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (15.0.1)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.18.0->optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (25.2.10)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.18.0->optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (6.33.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11->optimum[onnxruntime]) (3.0.3)\n","Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optimum[onnxruntime]) (2025.3.0)\n","Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optimum[onnxruntime]) (2024.2.0)\n","Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optimum[onnxruntime]) (2022.3.0)\n","Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->optimum[onnxruntime]) (1.4.0)\n","Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->optimum[onnxruntime]) (2024.2.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[onnxruntime]) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[onnxruntime]) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[onnxruntime]) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[onnxruntime]) (2025.10.5)\n","Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->optimum[onnxruntime]) (2024.2.0)\n","Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.18.0->optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (10.0)\n","Downloading optimum_onnx-0.0.3-py3-none-any.whl (192 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.3/192.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: optimum-onnx\n","Successfully installed optimum-onnx-0.0.3\n"]}],"source":["!pip uninstall -y optimum optimum-onnxruntime onnxruntime-gpu onnxruntime\n","!pip install --upgrade optimum onnxruntime\n","!pip install \"optimum[onnxruntime]\"\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-11-30T11:59:33.019283Z","iopub.status.busy":"2025-11-30T11:59:33.018951Z","iopub.status.idle":"2025-11-30T11:59:38.848173Z","shell.execute_reply":"2025-11-30T11:59:38.847426Z","shell.execute_reply.started":"2025-11-30T11:59:33.019256Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Skipping onnxruntime-gpu as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mFound existing installation: onnxruntime 1.23.2\n","Uninstalling onnxruntime-1.23.2:\n","  Successfully uninstalled onnxruntime-1.23.2\n","Collecting onnxruntime[cpu]\n","  Using cached onnxruntime-1.23.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n","\u001b[33mWARNING: onnxruntime 1.23.2 does not provide the extra 'cpu'\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime[cpu]) (15.0.1)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime[cpu]) (25.2.10)\n","Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime[cpu]) (1.26.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime[cpu]) (25.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime[cpu]) (6.33.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime[cpu]) (1.13.1)\n","Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime[cpu]) (1.3.8)\n","Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime[cpu]) (1.2.4)\n","Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime[cpu]) (0.1.1)\n","Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime[cpu]) (2025.3.0)\n","Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime[cpu]) (2022.3.0)\n","Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime[cpu]) (2.4.1)\n","Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime[cpu]) (10.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime[cpu]) (1.3.0)\n","Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.6->onnxruntime[cpu]) (2025.3.0)\n","Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.6->onnxruntime[cpu]) (2024.2.0)\n","Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.6->onnxruntime[cpu]) (2022.3.0)\n","Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.6->onnxruntime[cpu]) (1.4.0)\n","Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.6->onnxruntime[cpu]) (2024.2.0)\n","Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.6->onnxruntime[cpu]) (2024.2.0)\n","Using cached onnxruntime-1.23.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n","Installing collected packages: onnxruntime\n","Successfully installed onnxruntime-1.23.2\n"]}],"source":["!pip uninstall -y onnxruntime-gpu onnxruntime\n","!pip install onnxruntime[cpu]\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-11-30T12:19:13.879035Z","iopub.status.busy":"2025-11-30T12:19:13.878499Z","iopub.status.idle":"2025-11-30T12:19:13.885699Z","shell.execute_reply":"2025-11-30T12:19:13.884921Z","shell.execute_reply.started":"2025-11-30T12:19:13.879007Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU\n"]}],"source":["import onnxruntime as ort\n","print(ort.get_device())  # should print \"CPU\"\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-12-02T06:00:05.135804Z","iopub.status.busy":"2025-12-02T06:00:05.135523Z","iopub.status.idle":"2025-12-02T06:02:10.445175Z","shell.execute_reply":"2025-12-02T06:02:10.444419Z","shell.execute_reply.started":"2025-12-02T06:00:05.135784Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly\n","Ignoring torch: markers 'platform_machine == \"s390x\"' don't match your environment\n","Ignoring torch: markers 'platform_machine == \"s390x\"' don't match your environment\n","Requirement already satisfied: numpy~=1.26.4 in /usr/local/lib/python3.11/dist-packages (from -r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.26.4)\n","Requirement already satisfied: sentencepiece~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from -r ./requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.0)\n","Collecting transformers<5.0.0,>=4.57.1 (from -r ./requirements/requirements-convert_legacy_llama.txt (line 4))\n","  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gguf>=0.1.0 (from -r ./requirements/requirements-convert_legacy_llama.txt (line 6))\n","  Downloading https://download.pytorch.org/whl/nightly/gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n","Collecting protobuf<5.0.0,>=4.21.0 (from -r ./requirements/requirements-convert_legacy_llama.txt (line 7))\n","  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n","Requirement already satisfied: torch~=2.6.0 in /usr/local/lib/python3.11/dist-packages (from -r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (2.6.0+cu124)\n","Collecting aiohttp~=3.9.3 (from -r ./requirements/requirements-tool_bench.txt (line 1))\n","  Downloading https://download.pytorch.org/whl/nightly/aiohttp-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hCollecting pytest~=8.3.3 (from -r ./requirements/requirements-tool_bench.txt (line 2))\n","  Downloading pytest-8.3.5-py3-none-any.whl.metadata (7.6 kB)\n","Requirement already satisfied: huggingface_hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 3)) (0.36.0)\n","Collecting matplotlib~=3.10.0 (from -r ./requirements/requirements-tool_bench.txt (line 4))\n","  Downloading matplotlib-3.10.7-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n","Collecting openai~=1.55.3 (from -r ./requirements/requirements-tool_bench.txt (line 6))\n","  Downloading openai-1.55.3-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: pandas~=2.2.3 in /usr/local/lib/python3.11/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 7)) (2.2.3)\n","Collecting prometheus-client~=0.20.0 (from -r ./requirements/requirements-tool_bench.txt (line 8))\n","  Downloading prometheus_client-0.20.0-py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: requests~=2.32.3 in /usr/local/lib/python3.11/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 9)) (2.32.5)\n","Collecting wget~=3.2 (from -r ./requirements/requirements-tool_bench.txt (line 10))\n","  Downloading wget-3.2.zip (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting typer~=0.15.1 (from -r ./requirements/requirements-tool_bench.txt (line 11))\n","  Downloading typer-0.15.4-py3-none-any.whl.metadata (15 kB)\n","Collecting seaborn~=0.13.2 (from -r ./requirements/requirements-tool_bench.txt (line 12))\n","  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n","Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy~=1.26.4->-r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.3.8)\n","Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy~=1.26.4->-r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.2.4)\n","Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy~=1.26.4->-r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (0.1.1)\n","Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy~=1.26.4->-r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (2025.3.0)\n","Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy~=1.26.4->-r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (2022.3.0)\n","Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy~=1.26.4->-r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (2.4.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (3.20.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (6.0.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (2025.11.3)\n","Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4))\n","  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (4.67.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (4.15.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (2025.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (12.4.127)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5))\n","  Downloading https://download.pytorch.org/whl/nightly/cu124_pypi_pkg/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m118.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5))\n","  Downloading https://download.pytorch.org/whl/nightly/cu124_pypi_pkg/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5))\n","  Downloading https://download.pytorch.org/whl/nightly/cu124_pypi_pkg/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (10.3.5.147)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5))\n","  Downloading https://download.pytorch.org/whl/nightly/cu124_pypi_pkg/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5))\n","  Downloading https://download.pytorch.org/whl/nightly/cu124_pypi_pkg/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (1.3.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (6.7.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (1.22.0)\n","Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest~=8.3.3->-r ./requirements/requirements-tool_bench.txt (line 2)) (2.1.0)\n","Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest~=8.3.3->-r ./requirements/requirements-tool_bench.txt (line 2)) (1.6.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0,>=0.34.0->-r ./requirements/requirements-tool_bench.txt (line 3)) (1.2.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (4.59.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (11.3.0)\n","Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (2.9.0.post0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.10.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (2.12.4)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (1.3.1)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas~=2.2.3->-r ./requirements/requirements-tool_bench.txt (line 7)) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas~=2.2.3->-r ./requirements/requirements-tool_bench.txt (line 7)) (2025.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r ./requirements/requirements-tool_bench.txt (line 9)) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r ./requirements/requirements-tool_bench.txt (line 9)) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r ./requirements/requirements-tool_bench.txt (line 9)) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r ./requirements/requirements-tool_bench.txt (line 9)) (2025.10.5)\n","Collecting click<8.2,>=8.0.0 (from typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11))\n","  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (14.2.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.16.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (2.41.5)\n","Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.4.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (1.17.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (2.19.2)\n","Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.0->aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (0.4.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.0.3)\n","Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy~=1.26.4->-r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (2025.3.0)\n","Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy~=1.26.4->-r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (2024.2.0)\n","Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy~=1.26.4->-r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (2022.3.0)\n","Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy~=1.26.4->-r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.4.0)\n","Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy~=1.26.4->-r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (2024.2.0)\n","Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy~=1.26.4->-r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (2024.2.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (0.1.2)\n","Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m\n","\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/gguf-0.17.1-py3-none-any.whl (96 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pytest-8.3.5-py3-none-any.whl (343 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.6/343.6 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading matplotlib-3.10.7-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading openai-1.55.3-py3-none-any.whl (389 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading prometheus_client-0.20.0-py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typer-0.15.4-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=a0ecf097f417d1d7b6e167fcddb9954ad418f4038672215044158cc9dd8150ae\n","  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n","Successfully built wget\n","Installing collected packages: wget, pytest, protobuf, prometheus-client, nvidia-cusparse-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, click, nvidia-cusolver-cu12, nvidia-cudnn-cu12, aiohttp, typer, tokenizers, openai, matplotlib, transformers, seaborn, gguf\n","  Attempting uninstall: pytest\n","    Found existing installation: pytest 8.4.1\n","    Uninstalling pytest-8.4.1:\n","      Successfully uninstalled pytest-8.4.1\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 6.33.0\n","    Uninstalling protobuf-6.33.0:\n","      Successfully uninstalled protobuf-6.33.0\n","  Attempting uninstall: prometheus-client\n","    Found existing installation: prometheus_client 0.22.1\n","    Uninstalling prometheus_client-0.22.1:\n","      Successfully uninstalled prometheus_client-0.22.1\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: click\n","    Found existing installation: click 8.3.0\n","    Uninstalling click-8.3.0:\n","      Successfully uninstalled click-8.3.0\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: aiohttp\n","    Found existing installation: aiohttp 3.13.2\n","    Uninstalling aiohttp-3.13.2:\n","      Successfully uninstalled aiohttp-3.13.2\n","  Attempting uninstall: typer\n","    Found existing installation: typer 0.16.0\n","    Uninstalling typer-0.16.0:\n","      Successfully uninstalled typer-0.16.0\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.21.2\n","    Uninstalling tokenizers-0.21.2:\n","      Successfully uninstalled tokenizers-0.21.2\n","  Attempting uninstall: openai\n","    Found existing installation: openai 2.7.1\n","    Uninstalling openai-2.7.1:\n","      Successfully uninstalled openai-2.7.1\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.7.2\n","    Uninstalling matplotlib-3.7.2:\n","      Successfully uninstalled matplotlib-3.7.2\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.53.3\n","    Uninstalling transformers-4.53.3:\n","      Successfully uninstalled transformers-4.53.3\n","  Attempting uninstall: seaborn\n","    Found existing installation: seaborn 0.12.2\n","    Uninstalling seaborn-0.12.2:\n","      Successfully uninstalled seaborn-0.12.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n","opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n","ydata-profiling 4.17.0 requires matplotlib<=3.10,>=3.5, but you have matplotlib 3.10.7 which is incompatible.\n","s3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2025.10.0 which is incompatible.\n","a2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 4.25.8 which is incompatible.\n","preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\n","litellm 1.76.3 requires aiohttp>=3.10, but you have aiohttp 3.9.5 which is incompatible.\n","litellm 1.76.3 requires openai>=1.99.5, but you have openai 1.55.3 which is incompatible.\n","cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n","google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n","google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n","google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\n","bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n","libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n","gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\n","pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\n","pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\n","ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n","pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n","pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\n","jupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\n","grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\n","mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed aiohttp-3.9.5 click-8.1.8 gguf-0.17.1 matplotlib-3.10.7 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 openai-1.55.3 prometheus-client-0.20.0 protobuf-4.25.8 pytest-8.3.5 seaborn-0.13.2 tokenizers-0.22.1 transformers-4.57.3 typer-0.15.4 wget-3.2\n"]}],"source":["# !git clone https://github.com/ggerganov/llama.cpp\n","# !cd /kaggle/working/llama.cpp\n","!pip install -r requirements.txt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# !git clone https://huggingface.co/siddharth797/Qwen2.5-1.5b-full"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2025-12-02T06:12:39.509722Z","iopub.status.busy":"2025-12-02T06:12:39.509426Z","iopub.status.idle":"2025-12-02T06:13:08.403714Z","shell.execute_reply":"2025-12-02T06:13:08.403023Z","shell.execute_reply.started":"2025-12-02T06:12:39.509697Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO:hf-to-gguf:Loading model: Qwen2.5-1.5b-full\n","INFO:hf-to-gguf:Model architecture: Qwen2ForCausalLM\n","INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n","INFO:hf-to-gguf:gguf: indexing model part 'model-00002-of-00002.safetensors'\n","INFO:hf-to-gguf:gguf: indexing model part 'model-00001-of-00002.safetensors'\n","INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n","INFO:hf-to-gguf:Exporting model...\n","INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.22.attn_output.weight, torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.23.attn_output.weight, torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.24.attn_k.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.24.attn_output.weight, torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.24.attn_q.bias,        torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.24.attn_v.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.25.attn_k.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.25.attn_output.weight, torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.25.attn_q.bias,        torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.25.attn_v.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.26.attn_k.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.26.attn_output.weight, torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.26.attn_q.bias,        torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.26.attn_v.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.27.attn_k.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.27.attn_output.weight, torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.27.attn_q.bias,        torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.27.attn_v.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:output_norm.weight,        torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:token_embd.weight,         torch.float32 --> BF16, shape = {1536, 151936}\n","INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.18.attn_output.weight, torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.19.attn_output.weight, torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.20.attn_output.weight, torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.21.attn_output.weight, torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> BF16, shape = {8960, 1536}\n","INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> BF16, shape = {1536, 8960}\n","INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.float32 --> F32, shape = {1536}\n","INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float32 --> BF16, shape = {1536, 1536}\n","INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.float32 --> F32, shape = {256}\n","INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float32 --> BF16, shape = {1536, 256}\n","INFO:hf-to-gguf:Set meta model\n","INFO:hf-to-gguf:Set model parameters\n","INFO:hf-to-gguf:gguf: context length = 32768\n","INFO:hf-to-gguf:gguf: embedding length = 1536\n","INFO:hf-to-gguf:gguf: feed forward length = 8960\n","INFO:hf-to-gguf:gguf: head count = 12\n","INFO:hf-to-gguf:gguf: key-value head count = 2\n","INFO:hf-to-gguf:gguf: rope theta = 1000000.0\n","INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n","INFO:hf-to-gguf:gguf: file type = 32\n","INFO:hf-to-gguf:Set model quantization version\n","INFO:hf-to-gguf:Set model tokenizer\n","INFO:gguf.vocab:Adding 151387 merge(s).\n","INFO:gguf.vocab:Setting special token type eos to 151645\n","INFO:gguf.vocab:Setting special token type pad to 151643\n","INFO:gguf.vocab:Setting special token type bos to 151643\n","INFO:gguf.vocab:Setting add_bos_token to False\n","INFO:gguf.vocab:Setting chat_template to {%- if tools %}\n","    {{- '<|im_start|>system\\n' }}\n","    {%- if messages[0]['role'] == 'system' %}\n","        {{- messages[0]['content'] }}\n","    {%- else %}\n","        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n","    {%- endif %}\n","    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n","    {%- for tool in tools %}\n","        {{- \"\\n\" }}\n","        {{- tool | tojson }}\n","    {%- endfor %}\n","    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n","{%- else %}\n","    {%- if messages[0]['role'] == 'system' %}\n","        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n","    {%- else %}\n","        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n","    {%- endif %}\n","{%- endif %}\n","{%- for message in messages %}\n","    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n","        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n","    {%- elif message.role == \"assistant\" %}\n","        {{- '<|im_start|>' + message.role }}\n","        {%- if message.content %}\n","            {{- '\\n' + message.content }}\n","        {%- endif %}\n","        {%- for tool_call in message.tool_calls %}\n","            {%- if tool_call.function is defined %}\n","                {%- set tool_call = tool_call.function %}\n","            {%- endif %}\n","            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n","            {{- tool_call.name }}\n","            {{- '\", \"arguments\": ' }}\n","            {{- tool_call.arguments | tojson }}\n","            {{- '}\\n</tool_call>' }}\n","        {%- endfor %}\n","        {{- '<|im_end|>\\n' }}\n","    {%- elif message.role == \"tool\" %}\n","        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n","            {{- '<|im_start|>user' }}\n","        {%- endif %}\n","        {{- '\\n<tool_response>\\n' }}\n","        {{- message.content }}\n","        {{- '\\n</tool_response>' }}\n","        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n","            {{- '<|im_end|>\\n' }}\n","        {%- endif %}\n","    {%- endif %}\n","{%- endfor %}\n","{%- if add_generation_prompt %}\n","    {{- '<|im_start|>assistant\\n' }}\n","{%- endif %}\n","\n","INFO:gguf.gguf_writer:Writing the following files:\n","INFO:gguf.gguf_writer:Qwen3-8B-BF16.gguf: n_tensors = 338, total_size = 3.1G\n","Writing: 100%|███████████████████████████| 3.09G/3.09G [00:13<00:00, 236Mbyte/s]\n","INFO:hf-to-gguf:Model successfully exported to Qwen3-8B-BF16.gguf\n"]}],"source":["!python /kaggle/working/llama.cpp/convert_hf_to_gguf.py /kaggle/working/llama.cpp/Qwen2.5-1.5b-full --outtype bf16 --outfile Qwen3-8B-BF16.gguf"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2025-12-02T06:11:14.008237Z","iopub.status.busy":"2025-12-02T06:11:14.007397Z","iopub.status.idle":"2025-12-02T06:11:59.101356Z","shell.execute_reply":"2025-12-02T06:11:59.100442Z","shell.execute_reply.started":"2025-12-02T06:11:14.008202Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'Qwen2.5-1.5b-full'...\n","remote: Enumerating objects: 18, done.\u001b[K\n","remote: Counting objects: 100% (15/15), done.\u001b[K\n","remote: Compressing objects: 100% (15/15), done.\u001b[K\n","remote: Total 18 (delta 0), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n","Unpacking objects: 100% (18/18), 1.72 MiB | 4.28 MiB/s, done.\n","Filtering content: 100% (3/3), 1.76 GiB | 41.34 MiB/s, done.\n","Encountered 1 file(s) that may not have been copied correctly on Windows:\n","\tmodel-00001-of-00002.safetensors\n","\n","See: `git lfs help smudge` for more details.\n"]}],"source":[]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2025-12-02T06:14:59.058293Z","iopub.status.busy":"2025-12-02T06:14:59.057621Z","iopub.status.idle":"2025-12-02T06:14:59.173780Z","shell.execute_reply":"2025-12-02T06:14:59.173140Z","shell.execute_reply.started":"2025-12-02T06:14:59.058263Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/bin/bash: line 1: ./build/bin/llama-quantize: No such file or directory\n"]}],"source":["!./build/bin/llama-quantize -h\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2025-12-02T06:16:40.985014Z","iopub.status.busy":"2025-12-02T06:16:40.984392Z","iopub.status.idle":"2025-12-02T06:16:41.215007Z","shell.execute_reply":"2025-12-02T06:16:41.214175Z","shell.execute_reply.started":"2025-12-02T06:16:40.984984Z"},"trusted":true},"outputs":[],"source":["!mkdir build\n","!cd build"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2025-12-02T06:18:48.333924Z","iopub.status.busy":"2025-12-02T06:18:48.333151Z","iopub.status.idle":"2025-12-02T06:18:48.482653Z","shell.execute_reply":"2025-12-02T06:18:48.481796Z","shell.execute_reply.started":"2025-12-02T06:18:48.333889Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Makefile:6: *** Build system changed:\n"," The Makefile build has been replaced by CMake.\n","\n"," For build instructions see:\n"," https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md\n","\n",".  Stop.\n"]}],"source":["!make -j\n"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2025-12-02T06:20:57.399274Z","iopub.status.busy":"2025-12-02T06:20:57.398742Z","iopub.status.idle":"2025-12-02T06:20:57.847716Z","shell.execute_reply":"2025-12-02T06:20:57.846929Z","shell.execute_reply.started":"2025-12-02T06:20:57.399251Z"},"trusted":true},"outputs":[],"source":["LLAMA_DIR=\"/kaggle/working/llama.cpp\"\n","\n","\n","# 3) Clean any previous Makefile/build artifacts that conflict with CMake\n","!cd \"$LLAMA_DIR\"\n","# remove old Makefile if present (safe if from previous build system)\n","!rm -f Makefile\n","# remove any old build dir to start fresh\n","!rm -rf build\n","\n","# 4) Create build dir and run cmake from it\n","!mkdir -p build"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2025-12-02T06:27:30.135458Z","iopub.status.busy":"2025-12-02T06:27:30.135153Z","iopub.status.idle":"2025-12-02T06:27:30.471939Z","shell.execute_reply":"2025-12-02T06:27:30.470953Z","shell.execute_reply.started":"2025-12-02T06:27:30.135432Z"},"trusted":true},"outputs":[],"source":["# !rm -f Makefile\n","!rm -rf build\n","!mkdir build\n","!cd build"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2025-12-02T06:30:37.973126Z","iopub.status.busy":"2025-12-02T06:30:37.972309Z","iopub.status.idle":"2025-12-02T06:35:41.175546Z","shell.execute_reply":"2025-12-02T06:35:41.174649Z","shell.execute_reply.started":"2025-12-02T06:30:37.973092Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n","Get:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]           \n","Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease                         \n","Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \n","Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,157 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \n","Get:8 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.6 kB]\n","Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,498 kB]  \n","Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n","Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]     \n","Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,839 kB]\n","Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,008 kB]\n","Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease   \n","Get:16 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,222 kB]\n","Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,286 kB]\n","Get:19 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,535 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,870 kB]\n","Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,592 kB]\n","Fetched 37.5 MB in 3s (14.0 MB/s)                              \n","Reading package lists... Done\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","build-essential is already the newest version (12.9ubuntu3).\n","libopenblas-dev is already the newest version (0.3.20+ds-1).\n","cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n","The following packages were automatically installed and are no longer required:\n","  libpkgconf3 libreadline-dev\n","Use 'sudo apt autoremove' to remove them.\n","Suggested packages:\n","  libomp-14-doc elpa-ess r-doc-info | r-doc-pdf r-mathlib r-base-html\n","Recommended packages:\n","  r-base-dev r-doc-html\n","The following packages will be REMOVED:\n","  pkgconf r-base-dev\n","The following NEW packages will be installed:\n","  libomp-14-dev libomp-dev libomp5-14 pkg-config\n","The following packages will be upgraded:\n","  r-base-core\n","1 upgraded, 4 newly installed, 2 to remove and 178 not upgraded.\n","Need to get 29.9 MB of archives.\n","After this operation, 9,039 kB of additional disk space will be used.\n","Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ r-base-core 4.5.2-1.2204.0 [29.1 MB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 pkg-config amd64 0.29.2-1ubuntu3 [48.2 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libomp5-14 amd64 1:14.0.0-1ubuntu1.1 [389 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libomp-14-dev amd64 1:14.0.0-1ubuntu1.1 [347 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libomp-dev amd64 1:14.0-55~exp2 [3,074 B]\n","Fetched 29.9 MB in 1s (29.2 MB/s)     \n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 5.)\n","debconf: falling back to frontend: Readline\n","(Reading database ... 128639 files and directories currently installed.)\n","Removing r-base-dev (4.5.1-1.2204.0) ...\n","dpkg: pkgconf: dependency problems, but removing anyway as you requested:\n"," libsndfile1-dev:amd64 depends on pkg-config; however:\n","  Package pkg-config is not installed.\n","  Package pkgconf which provides pkg-config is to be removed.\n"," libopencv-dev depends on pkg-config; however:\n","  Package pkg-config is not installed.\n","  Package pkgconf which provides pkg-config is to be removed.\n"," libmkl-dev:amd64 depends on pkg-config; however:\n","  Package pkg-config is not installed.\n","  Package pkgconf which provides pkg-config is to be removed.\n"," libmagickwand-6.q16-dev:amd64 depends on pkg-config; however:\n","  Package pkg-config is not installed.\n","  Package pkgconf which provides pkg-config is to be removed.\n"," libmagickcore-6.q16-dev:amd64 depends on pkg-config; however:\n","  Package pkg-config is not installed.\n","  Package pkgconf which provides pkg-config is to be removed.\n"," libjack-dev depends on pkg-config; however:\n","  Package pkg-config is not installed.\n","  Package pkgconf which provides pkg-config is to be removed.\n"," libgphoto2-dev:amd64 depends on pkg-config; however:\n","  Package pkg-config is not installed.\n","  Package pkgconf which provides pkg-config is to be removed.\n"," libglib2.0-dev:amd64 depends on pkg-config; however:\n","  Package pkg-config is not installed.\n","  Package pkgconf which provides pkg-config is to be removed.\n"," libfontconfig-dev:amd64 depends on pkg-config; however:\n","  Package pkg-config is not installed.\n","  Package pkgconf which provides pkg-config is to be removed.\n","\n","Removing pkgconf (1.8.0-1) ...\n","Removing 'diversion of /usr/bin/pkg-config to /usr/bin/pkg-config.real by pkgconf'\n","Removing 'diversion of /usr/share/aclocal/pkg.m4 to /usr/share/aclocal/pkg.real.m4 by pkgconf'\n","Removing 'diversion of /usr/share/man/man1/pkg-config.1.gz to /usr/share/man/man1/pkg-config.real.1.gz by pkgconf'\n","Removing 'diversion of /usr/share/pkg-config-crosswrapper to /usr/share/pkg-config-crosswrapper.real by pkgconf'\n","Selecting previously unselected package pkg-config.\n","(Reading database ... 128615 files and directories currently installed.)\n","Preparing to unpack .../pkg-config_0.29.2-1ubuntu3_amd64.deb ...\n","Unpacking pkg-config (0.29.2-1ubuntu3) ...\n","Selecting previously unselected package libomp5-14:amd64.\n","Preparing to unpack .../libomp5-14_1%3a14.0.0-1ubuntu1.1_amd64.deb ...\n","Unpacking libomp5-14:amd64 (1:14.0.0-1ubuntu1.1) ...\n","Selecting previously unselected package libomp-14-dev.\n","Preparing to unpack .../libomp-14-dev_1%3a14.0.0-1ubuntu1.1_amd64.deb ...\n","Unpacking libomp-14-dev (1:14.0.0-1ubuntu1.1) ...\n","Preparing to unpack .../r-base-core_4.5.2-1.2204.0_amd64.deb ...\n","Unpacking r-base-core (4.5.2-1.2204.0) over (4.5.1-1.2204.0) ...\n","Selecting previously unselected package libomp-dev:amd64.\n","Preparing to unpack .../libomp-dev_1%3a14.0-55~exp2_amd64.deb ...\n","Unpacking libomp-dev:amd64 (1:14.0-55~exp2) ...\n","Setting up libomp5-14:amd64 (1:14.0.0-1ubuntu1.1) ...\n","Setting up r-base-core (4.5.2-1.2204.0) ...\n","Installing new version of config file /etc/R/Makeconf ...\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n","debconf: falling back to frontend: Readline\n","Setting up pkg-config (0.29.2-1ubuntu3) ...\n","Setting up libomp-14-dev (1:14.0.0-1ubuntu1.1) ...\n","Setting up libomp-dev:amd64 (1:14.0-55~exp2) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n","/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n","Processing triggers for hicolor-icon-theme (0.17-2) ...\n","Switching to llama.cpp directory\n","/kaggle/working/llama.cpp\n","/kaggle/working/llama.cpp/build\n","-- The C compiler identification is GNU 11.4.0\n","-- The CXX compiler identification is GNU 11.4.0\n","-- Detecting C compiler ABI info\n","-- Detecting C compiler ABI info - done\n","-- Check for working C compiler: /usr/bin/gcc - skipped\n","-- Detecting C compile features\n","-- Detecting C compile features - done\n","-- Detecting CXX compiler ABI info\n","-- Detecting CXX compiler ABI info - done\n","-- Check for working CXX compiler: /usr/bin/g++ - skipped\n","-- Detecting CXX compile features\n","-- Detecting CXX compile features - done\n","\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n","-- Found Git: /usr/bin/git (found version \"2.34.1\")\n","-- The ASM compiler identification is GNU\n","-- Found assembler: /usr/bin/gcc\n","-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n","-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n","-- Found Threads: TRUE\n","-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n","-- CMAKE_SYSTEM_PROCESSOR: x86_64\n","-- GGML_SYSTEM_ARCH: x86\n","-- Including CPU backend\n","-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n","-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n","-- Found OpenMP: TRUE (found version \"4.5\")\n","-- x86 detected\n","-- Adding CPU backend variant ggml-cpu: -march=native \n","-- ggml version: 0.9.4\n","-- ggml commit:  ed3208992\n","-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n","-- Configuring done (2.4s)\n","-- Generating done (0.3s)\n","-- Build files have been written to: /kaggle/working/llama.cpp/build\n","[  1%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n","[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n","[  2%] \u001b[32mBuilding CXX object vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/httplib.cpp.o\u001b[0m\n","[  2%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n","[  2%] Built target build_info\n","[  3%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n","[  3%] Built target sha256\n","[  4%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n","[  4%] Built target sha1\n","[  4%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\u001b[0m\n","[  4%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n","[  4%] Built target llama-llava-cli\n","[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n","[  4%] Built target xxhash\n","[  4%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\u001b[0m\n","[  4%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\u001b[0m\n","[  4%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n","[  4%] Built target llama-gemma3-cli\n","[  4%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\u001b[0m\n","[  4%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n","[  4%] Built target llama-minicpmv-cli\n","[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n","[  4%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n","[  4%] Built target llama-qwen2vl-cli\n","[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n","[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n","[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n","[  5%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n","[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n","[  6%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n","[  6%] Built target ggml-base\n","[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n","[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n","[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n","[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n","[  7%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n","[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n","[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n","[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n","[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n","[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n","[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n","[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n","[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n","[ 10%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n","[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n","[ 10%] \u001b[32m\u001b[1mLinking CXX static library libcpp-httplib.a\u001b[0m\n","[ 10%] Built target cpp-httplib\n","[ 10%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n","[ 10%] Built target ggml-cpu\n","[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n","[ 10%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n","[ 10%] Built target ggml\n","[ 10%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n","[ 10%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n","[ 11%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n","[ 11%] Built target llama-gguf\n","[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n","[ 12%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n","[ 12%] Built target llama-gguf-hash\n","[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\n","[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n","[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n","[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n","[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n","[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n","[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n","[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\u001b[0m\n","[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n","[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n","[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\n","[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n","[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n","[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n","[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n","[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n","[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n","[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/afmoe.cpp.o\u001b[0m\n","[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/apertus.cpp.o\u001b[0m\n","[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arcee.cpp.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arctic.cpp.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arwkv7.cpp.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/baichuan.cpp.o\u001b[0m\n","[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o\u001b[0m\n","[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o\u001b[0m\n","[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bert.cpp.o\u001b[0m\n","[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bitnet.cpp.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bloom.cpp.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chameleon.cpp.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chatglm.cpp.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/codeshell.cpp.o\u001b[0m\n","[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cogvlm.cpp.o\u001b[0m\n","[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o\u001b[0m\n","[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/command-r.cpp.o\u001b[0m\n","[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dbrx.cpp.o\u001b[0m\n","[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deci.cpp.o\u001b[0m\n","[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek.cpp.o\u001b[0m\n","[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek2.cpp.o\u001b[0m\n","[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dots1.cpp.o\u001b[0m\n","[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dream.cpp.o\u001b[0m\n","[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o\u001b[0m\n","[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o\u001b[0m\n","[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone.cpp.o\u001b[0m\n","[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone4.cpp.o\u001b[0m\n","[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon.cpp.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma.cpp.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3-iswa.cpp.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o\u001b[0m\n","[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4.cpp.o\u001b[0m\n","[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gpt2.cpp.o\u001b[0m\n","[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gptneox.cpp.o\u001b[0m\n","[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite.cpp.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grok.cpp.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grovemoe.cpp.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/internlm2.cpp.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jais.cpp.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jamba.cpp.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/lfm2.cpp.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada-moe.cpp.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada.cpp.o\u001b[0m\n","[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o\u001b[0m\n","[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama.cpp.o\u001b[0m\n","[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mamba.cpp.o\u001b[0m\n","[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minicpm3.cpp.o\u001b[0m\n","[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o\u001b[0m\n","[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mpt.cpp.o\u001b[0m\n","[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o\u001b[0m\n","[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron.cpp.o\u001b[0m\n","[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/neo-bert.cpp.o\u001b[0m\n","[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo.cpp.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo2.cpp.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmoe.cpp.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openelm.cpp.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/orion.cpp.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi2.cpp.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi3.cpp.o\u001b[0m\n","[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo.cpp.o\u001b[0m\n","[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo2.cpp.o\u001b[0m\n","[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plm.cpp.o\u001b[0m\n","[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen.cpp.o\u001b[0m\n","[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2.cpp.o\u001b[0m\n","[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o\u001b[0m\n","[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o\u001b[0m\n","[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3.cpp.o\u001b[0m\n","[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o\u001b[0m\n","[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3next.cpp.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/refact.cpp.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rnd1.cpp.o\u001b[0m\n","[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o\u001b[0m\n","[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6.cpp.o\u001b[0m\n","[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o\u001b[0m\n","[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o\u001b[0m\n","[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7.cpp.o\u001b[0m\n","[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/seed-oss.cpp.o\u001b[0m\n","[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smallthinker.cpp.o\u001b[0m\n","[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smollm3.cpp.o\u001b[0m\n","[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/stablelm.cpp.o\u001b[0m\n","[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder.cpp.o\u001b[0m\n","[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder2.cpp.o\u001b[0m\n","[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-dec.cpp.o\u001b[0m\n","[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-enc.cpp.o\u001b[0m\n","[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/xverse.cpp.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mistral3.cpp.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/graph-context-mamba.cpp.o\u001b[0m\n","[ 46%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n","[ 46%] Built target llama\n","[ 46%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n","[ 46%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n","[ 47%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n","[ 47%] Built target test-c\n","[ 47%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o\u001b[0m\n","[ 48%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n","[ 48%] Built target llama-simple\n","[ 48%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n","[ 48%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n","[ 48%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n","[ 48%] Built target llama-simple-chat\n","[ 49%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser-xml-toolcall.cpp.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n","[ 51%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/download.cpp.o\u001b[0m\n","[ 51%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n","[ 51%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd.so\u001b[0m\n","[ 51%] Built target mtmd\n","[ 51%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n","[ 52%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n","[ 52%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n","[ 52%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n","[ 52%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n","[ 53%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n","[ 53%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n","[ 53%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n","[ 53%] Built target common\n","[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n","[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n","[ 55%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n","[ 56%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n","[ 56%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n","[ 56%] Built target test-tokenizer-0\n","[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n","[ 56%] Built target test-sampling\n","[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n","[ 56%] Built target test-grammar-parser\n","[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n","[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n","[ 57%] Built target test-llama-grammar\n","[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n","[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-stats.dir/test-quantize-stats.cpp.o\u001b[0m\n","[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n","[ 58%] Built target test-grammar-integration\n","[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gbnf-validator.dir/test-gbnf-validator.cpp.o\u001b[0m\n","[ 59%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gbnf-validator\u001b[0m\n","[ 59%] Built target test-gbnf-validator\n","[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n","[ 60%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n","[ 60%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-stats\u001b[0m\n","[ 60%] Built target test-tokenizer-1-bpe\n","[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n","[ 60%] Built target test-quantize-stats\n","[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/test-chat-parser.cpp.o\u001b[0m\n","[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n","[ 61%] Built target test-tokenizer-1-spm\n","[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/get-model.cpp.o\u001b[0m\n","[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n","[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n","[ 62%] Built target test-json-schema-to-grammar\n","[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n","[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/test-json-partial.cpp.o\u001b[0m\n","[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n","[ 62%] Built target test-chat-template\n","[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/get-model.cpp.o\u001b[0m\n","[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n","[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n","[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n","[ 63%] Built target test-log\n","[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/test-regex-partial.cpp.o\u001b[0m\n","[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-partial\u001b[0m\n","[ 63%] Built target test-json-partial\n","[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/get-model.cpp.o\u001b[0m\n","[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/test-thread-safety.cpp.o\u001b[0m\n","[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-regex-partial\u001b[0m\n","[ 64%] Built target test-regex-partial\n","[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/get-model.cpp.o\u001b[0m\n","[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n","[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-thread-safety\u001b[0m\n","[ 65%] Built target test-thread-safety\n","[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n","[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/test-opt.cpp.o\u001b[0m\n","[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n","[ 65%] Built target test-arg-parser\n","[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/get-model.cpp.o\u001b[0m\n","[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n","[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-parser\u001b[0m\n","[ 66%] Built target test-chat-parser\n","[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n","[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n","[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-opt\u001b[0m\n","[ 67%] Built target test-opt\n","[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n","[ 68%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n","[ 68%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n","[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n","[ 68%] Built target test-model-load-cancel\n","[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n","[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n","[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n","[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n","[ 69%] Built target test-gguf\n","[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n","[ 69%] Built target test-autorelease\n","[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n","[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n","[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n","[ 70%] Built target test-barrier\n","[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n","[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n","[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n","[ 71%] Built target test-quantize-fns\n","[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n","[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n","[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n","[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n","[ 72%] Built target test-rope\n","[ 73%] \u001b[32mBuilding C object tests/CMakeFiles/test-mtmd-c-api.dir/test-mtmd-c-api.c.o\u001b[0m\n","[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-mtmd-c-api.dir/get-model.cpp.o\u001b[0m\n","[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-mtmd-c-api\u001b[0m\n","[ 73%] Built target test-mtmd-c-api\n","[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/test-alloc.cpp.o\u001b[0m\n","[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n","[ 74%] Built target test-quantize-perf\n","[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/get-model.cpp.o\u001b[0m\n","[ 75%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n","[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-alloc\u001b[0m\n","[ 76%] Built target test-alloc\n","[ 77%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n","[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n","[ 77%] Built target llama-batched\n","[ 77%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n","[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n","[ 77%] Built target llama-embedding\n","[ 77%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n","[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n","[ 77%] Built target llama-eval-callback\n","[ 77%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n","[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n","[ 78%] Built target llama-lookahead\n","[ 78%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n","[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n","[ 78%] Built target llama-lookup\n","[ 78%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n","[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n","[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n","[ 79%] Built target llama-lookup-create\n","[ 80%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n","[ 80%] Built target llama-lookup-merge\n","[ 80%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n","[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n","[ 80%] Built target llama-lookup-stats\n","[ 81%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n","[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n","[ 81%] Built target llama-parallel\n","[ 82%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n","[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n","[ 82%] Built target test-chat\n","[ 82%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n","[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n","[ 82%] Built target llama-passkey\n","[ 82%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n","[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n","[ 82%] Built target llama-save-load-state\n","[ 82%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n","[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n","[ 82%] Built target llama-retrieval\n","[ 82%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n","[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n","[ 82%] Built target llama-speculative-simple\n","[ 82%] \u001b[32mBuilding CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\u001b[0m\n","[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n","[ 83%] Built target llama-gen-docs\n","[ 83%] \u001b[32mBuilding CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\u001b[0m\n","[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n","[ 84%] Built target llama-speculative\n","[ 85%] \u001b[32mBuilding CXX object examples/model-conversion/CMakeFiles/llama-logits.dir/logits.cpp.o\u001b[0m\n","[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-finetune\u001b[0m\n","[ 86%] Built target llama-finetune\n","[ 86%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n","[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-logits\u001b[0m\n","[ 86%] Built target llama-logits\n","[ 87%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n","[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n","[ 87%] Built target llama-vdot\n","[ 87%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n","[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-diffusion-cli\u001b[0m\n","[ 87%] Built target llama-diffusion-cli\n","[ 87%] \u001b[32mBuilding CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n","[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n","[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n","[ 87%] Built target llama-convert-llama2c-to-ggml\n","[ 87%] \u001b[32mBuilding CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n","[ 87%] Built target llama-q8dot\n","[ 88%] \u001b[32mBuilding CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n","[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n","[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n","[ 89%] Built target llama-gguf-split\n","[ 89%] \u001b[32mBuilding CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n","[ 89%] Built target llama-batched-bench\n","[ 89%] \u001b[32mBuilding CXX object tools/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n","[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n","[ 89%] Built target test-backend-ops\n","[ 89%] \u001b[32mBuilding CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n","[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n","[ 90%] Built target llama-cli\n","[ 90%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n","[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n","[ 91%] Built target llama-quantize\n","[ 92%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n","[ 92%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n","[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n","[ 93%] Built target llama-perplexity\n","[ 93%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n","[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n","[ 93%] Built target llama-imatrix\n","[ 93%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n","[ 93%] \u001b[32mBuilding CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n","[ 93%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n","[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n","[ 94%] Built target llama-tokenize\n","[ 94%] \u001b[32mBuilding CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n","[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n","[ 94%] Built target llama-bench\n","[ 94%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-http.cpp.o\u001b[0m\n","[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\n","[ 95%] Built target llama-run\n","[ 96%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-models.cpp.o\u001b[0m\n","[ 96%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-task.cpp.o\u001b[0m\n","[ 97%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\u001b[0m\n","[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n","[ 97%] Built target llama-tts\n","[ 97%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-queue.cpp.o\u001b[0m\n","[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-mtmd-cli\u001b[0m\n","[ 97%] Built target llama-mtmd-cli\n","[ 97%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-common.cpp.o\u001b[0m\n","[ 97%] \u001b[32mBuilding CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n","[ 98%] \u001b[32mBuilding CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n","[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n","[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n","[ 99%] Built target llama-export-lora\n","[100%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-context.cpp.o\u001b[0m\n","[100%] Built target llama-cvector-generator\n","[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n","[100%] Built target llama-server\n","Built binaries in build/bin:\n","total 98584\n","drwxr-xr-x  2 root root    4096 Dec  2 06:35 .\n","drwxr-xr-x 13 root root    4096 Dec  2 06:31 ..\n","lrwxrwxrwx  1 root root      17 Dec  2 06:31 libggml-base.so -> libggml-base.so.0\n","lrwxrwxrwx  1 root root      21 Dec  2 06:31 libggml-base.so.0 -> libggml-base.so.0.9.4\n","-rwxr-xr-x  1 root root  733464 Dec  2 06:31 libggml-base.so.0.9.4\n","lrwxrwxrwx  1 root root      16 Dec  2 06:31 libggml-cpu.so -> libggml-cpu.so.0\n","lrwxrwxrwx  1 root root      20 Dec  2 06:31 libggml-cpu.so.0 -> libggml-cpu.so.0.9.4\n","-rwxr-xr-x  1 root root 1066992 Dec  2 06:31 libggml-cpu.so.0.9.4\n","lrwxrwxrwx  1 root root      12 Dec  2 06:31 libggml.so -> libggml.so.0\n","lrwxrwxrwx  1 root root      16 Dec  2 06:31 libggml.so.0 -> libggml.so.0.9.4\n","-rwxr-xr-x  1 root root   55232 Dec  2 06:31 libggml.so.0.9.4\n","lrwxrwxrwx  1 root root      13 Dec  2 06:32 libllama.so -> libllama.so.0\n","lrwxrwxrwx  1 root root      20 Dec  2 06:32 libllama.so.0 -> libllama.so.0.0.7225\n","-rwxr-xr-x  1 root root 2732824 Dec  2 06:32 libllama.so.0.0.7225\n","lrwxrwxrwx  1 root root      12 Dec  2 06:33 libmtmd.so -> libmtmd.so.0\n","lrwxrwxrwx  1 root root      19 Dec  2 06:33 libmtmd.so.0 -> libmtmd.so.0.0.7225\n","-rwxr-xr-x  1 root root  803912 Dec  2 06:33 libmtmd.so.0.0.7225\n","-rwxr-xr-x  1 root root 2747176 Dec  2 06:34 llama-batched\n","-rwxr-xr-x  1 root root 2747216 Dec  2 06:34 llama-batched-bench\n","-rwxr-xr-x  1 root root  584064 Dec  2 06:35 llama-bench\n","-rwxr-xr-x  1 root root 2786984 Dec  2 06:34 llama-cli\n","-rwxr-xr-x  1 root root  413232 Dec  2 06:34 llama-convert-llama2c-to-ggml\n","-rwxr-xr-x  1 root root 2780360 Dec  2 06:35 llama-cvector-generator\n","-rwxr-xr-x  1 root root 2764440 Dec  2 06:34 llama-diffusion-cli\n","-rwxr-xr-x  1 root root 2760560 Dec  2 06:34 llama-embedding\n","-rwxr-xr-x  1 root root 2747472 Dec  2 06:34 llama-eval-callback\n","-rwxr-xr-x  1 root root 2785080 Dec  2 06:35 llama-export-lora\n","-rwxr-xr-x  1 root root 2743272 Dec  2 06:34 llama-finetune\n","-rwxr-xr-x  1 root root   16904 Dec  2 06:31 llama-gemma3-cli\n","-rwxr-xr-x  1 root root 2747344 Dec  2 06:34 llama-gen-docs\n","-rwxr-xr-x  1 root root   28248 Dec  2 06:31 llama-gguf\n","-rwxr-xr-x  1 root root  103448 Dec  2 06:31 llama-gguf-hash\n","-rwxr-xr-x  1 root root   48152 Dec  2 06:34 llama-gguf-split\n","-rwxr-xr-x  1 root root 2850176 Dec  2 06:34 llama-imatrix\n","-rwxr-xr-x  1 root root   16904 Dec  2 06:31 llama-llava-cli\n","-rwxr-xr-x  1 root root  380288 Dec  2 06:34 llama-logits\n","-rwxr-xr-x  1 root root 2751768 Dec  2 06:34 llama-lookahead\n","-rwxr-xr-x  1 root root 2776816 Dec  2 06:34 llama-lookup\n","-rwxr-xr-x  1 root root 2768032 Dec  2 06:34 llama-lookup-create\n","-rwxr-xr-x  1 root root   74880 Dec  2 06:34 llama-lookup-merge\n","-rwxr-xr-x  1 root root 2772928 Dec  2 06:34 llama-lookup-stats\n","-rwxr-xr-x  1 root root   16904 Dec  2 06:31 llama-minicpmv-cli\n","-rwxr-xr-x  1 root root 2770512 Dec  2 06:35 llama-mtmd-cli\n","-rwxr-xr-x  1 root root 2764264 Dec  2 06:34 llama-parallel\n","-rwxr-xr-x  1 root root 2747224 Dec  2 06:34 llama-passkey\n","-rwxr-xr-x  1 root root 2840808 Dec  2 06:34 llama-perplexity\n","-rwxr-xr-x  1 root root   21192 Dec  2 06:34 llama-q8dot\n","-rwxr-xr-x  1 root root  428776 Dec  2 06:34 llama-quantize\n","-rwxr-xr-x  1 root root   16904 Dec  2 06:31 llama-qwen2vl-cli\n","-rwxr-xr-x  1 root root 2765152 Dec  2 06:34 llama-retrieval\n","-rwxr-xr-x  1 root root 2275728 Dec  2 06:35 llama-run\n","-rwxr-xr-x  1 root root 2747656 Dec  2 06:34 llama-save-load-state\n","-rwxr-xr-x  1 root root 6018592 Dec  2 06:35 llama-server\n","-rwxr-xr-x  1 root root   27088 Dec  2 06:32 llama-simple\n","-rwxr-xr-x  1 root root   32456 Dec  2 06:32 llama-simple-chat\n","-rwxr-xr-x  1 root root 2775352 Dec  2 06:34 llama-speculative\n","-rwxr-xr-x  1 root root 2764768 Dec  2 06:34 llama-speculative-simple\n","-rwxr-xr-x  1 root root  384576 Dec  2 06:34 llama-tokenize\n","-rwxr-xr-x  1 root root 2855912 Dec  2 06:35 llama-tts\n","-rwxr-xr-x  1 root root   21776 Dec  2 06:34 llama-vdot\n","-rwxr-xr-x  1 root root   50328 Dec  2 06:34 test-alloc\n","-rwxr-xr-x  1 root root 2756096 Dec  2 06:34 test-arg-parser\n","-rwxr-xr-x  1 root root   18144 Dec  2 06:34 test-autorelease\n","-rwxr-xr-x  1 root root  979472 Dec  2 06:34 test-backend-ops\n","-rwxr-xr-x  1 root root   22160 Dec  2 06:34 test-barrier\n","-rwxr-xr-x  1 root root   15776 Dec  2 06:32 test-c\n","-rwxr-xr-x  1 root root 2698088 Dec  2 06:34 test-chat\n","-rwxr-xr-x  1 root root 2169000 Dec  2 06:34 test-chat-parser\n","-rwxr-xr-x  1 root root 2172760 Dec  2 06:34 test-chat-template\n","-rwxr-xr-x  1 root root   26864 Dec  2 06:34 test-gbnf-validator\n","-rwxr-xr-x  1 root root   77256 Dec  2 06:34 test-gguf\n","-rwxr-xr-x  1 root root  805736 Dec  2 06:33 test-grammar-integration\n","-rwxr-xr-x  1 root root   41072 Dec  2 06:33 test-grammar-parser\n","-rwxr-xr-x  1 root root  396672 Dec  2 06:34 test-json-partial\n","-rwxr-xr-x  1 root root  803224 Dec  2 06:34 test-json-schema-to-grammar\n","-rwxr-xr-x  1 root root   46336 Dec  2 06:33 test-llama-grammar\n","-rwxr-xr-x  1 root root   43240 Dec  2 06:34 test-log\n","-rwxr-xr-x  1 root root   16512 Dec  2 06:34 test-model-load-cancel\n","-rwxr-xr-x  1 root root   16944 Dec  2 06:34 test-mtmd-c-api\n","-rwxr-xr-x  1 root root   68128 Dec  2 06:34 test-opt\n","-rwxr-xr-x  1 root root   17568 Dec  2 06:34 test-quantize-fns\n","-rwxr-xr-x  1 root root   41656 Dec  2 06:34 test-quantize-perf\n","-rwxr-xr-x  1 root root  217376 Dec  2 06:34 test-quantize-stats\n","-rwxr-xr-x  1 root root  457264 Dec  2 06:34 test-regex-partial\n","-rwxr-xr-x  1 root root   21472 Dec  2 06:34 test-rope\n","-rwxr-xr-x  1 root root   50272 Dec  2 06:33 test-sampling\n","-rwxr-xr-x  1 root root 2748216 Dec  2 06:34 test-thread-safety\n","-rwxr-xr-x  1 root root  399528 Dec  2 06:33 test-tokenizer-0\n","-rwxr-xr-x  1 root root  380928 Dec  2 06:34 test-tokenizer-1-bpe\n","-rwxr-xr-x  1 root root  376568 Dec  2 06:34 test-tokenizer-1-spm\n","usage: ./bin/llama-quantize [--help] [--allow-requantize] [--leave-output-tensor] [--pure] [--imatrix] [--include-weights]\n","       [--exclude-weights] [--output-tensor-type] [--token-embedding-type] [--tensor-type] [--prune-layers] [--keep-split] [--override-kv]\n","       model-f32.gguf [model-quant.gguf] type [nthreads]\n","\n","  --allow-requantize: Allows requantizing tensors that have already been quantized. Warning: This can severely reduce quality compared to quantizing from 16bit or 32bit\n","  --leave-output-tensor: Will leave output.weight un(re)quantized. Increases model size but may also increase quality, especially when requantizing\n","  --pure: Disable k-quant mixtures and quantize all tensors to the same type\n","  --imatrix file_name: use data in file_name as importance matrix for quant optimizations\n","  --include-weights tensor_name: use importance matrix for this/these tensor(s)\n","  --exclude-weights tensor_name: use importance matrix for this/these tensor(s)\n","  --output-tensor-type ggml_type: use this ggml_type for the output.weight tensor\n","  --token-embedding-type ggml_type: use this ggml_type for the token embeddings tensor\n","  --tensor-type TENSOR=TYPE: quantize this tensor to this ggml_type. example: --tensor-type attn_q=q8_0\n","      Advanced option to selectively quantize tensors. May be specified multiple times.\n","  --prune-layers L0,L1,L2...comma-separated list of layer numbers to prune from the model\n","      Advanced option to remove all tensors from the given layers\n","  --keep-split: will generate quantized model in the same shards as input\n","  --override-kv KEY=TYPE:VALUE\n","      Advanced option to override model metadata by key in the quantized model. May be specified multiple times.\n","Note: --include-weights and --exclude-weights cannot be used together\n","\n","Allowed quantization types:\n","   2  or  Q4_0    :  4.34G, +0.4685 ppl @ Llama-3-8B\n","   3  or  Q4_1    :  4.78G, +0.4511 ppl @ Llama-3-8B\n","  38  or  MXFP4_MOE :  MXFP4 MoE\n","   8  or  Q5_0    :  5.21G, +0.1316 ppl @ Llama-3-8B\n","   9  or  Q5_1    :  5.65G, +0.1062 ppl @ Llama-3-8B\n","  19  or  IQ2_XXS :  2.06 bpw quantization\n","  20  or  IQ2_XS  :  2.31 bpw quantization\n","  28  or  IQ2_S   :  2.5  bpw quantization\n","  29  or  IQ2_M   :  2.7  bpw quantization\n","  24  or  IQ1_S   :  1.56 bpw quantization\n","  31  or  IQ1_M   :  1.75 bpw quantization\n","  36  or  TQ1_0   :  1.69 bpw ternarization\n","  37  or  TQ2_0   :  2.06 bpw ternarization\n","  10  or  Q2_K    :  2.96G, +3.5199 ppl @ Llama-3-8B\n","  21  or  Q2_K_S  :  2.96G, +3.1836 ppl @ Llama-3-8B\n","  23  or  IQ3_XXS :  3.06 bpw quantization\n","  26  or  IQ3_S   :  3.44 bpw quantization\n","  27  or  IQ3_M   :  3.66 bpw quantization mix\n","  12  or  Q3_K    : alias for Q3_K_M\n","  22  or  IQ3_XS  :  3.3 bpw quantization\n","  11  or  Q3_K_S  :  3.41G, +1.6321 ppl @ Llama-3-8B\n","  12  or  Q3_K_M  :  3.74G, +0.6569 ppl @ Llama-3-8B\n","  13  or  Q3_K_L  :  4.03G, +0.5562 ppl @ Llama-3-8B\n","  25  or  IQ4_NL  :  4.50 bpw non-linear quantization\n","  30  or  IQ4_XS  :  4.25 bpw non-linear quantization\n","  15  or  Q4_K    : alias for Q4_K_M\n","  14  or  Q4_K_S  :  4.37G, +0.2689 ppl @ Llama-3-8B\n","  15  or  Q4_K_M  :  4.58G, +0.1754 ppl @ Llama-3-8B\n","  17  or  Q5_K    : alias for Q5_K_M\n","  16  or  Q5_K_S  :  5.21G, +0.1049 ppl @ Llama-3-8B\n","  17  or  Q5_K_M  :  5.33G, +0.0569 ppl @ Llama-3-8B\n","  18  or  Q6_K    :  6.14G, +0.0217 ppl @ Llama-3-8B\n","   7  or  Q8_0    :  7.96G, +0.0026 ppl @ Llama-3-8B\n","   1  or  F16     : 14.00G, +0.0020 ppl @ Mistral-7B\n","  32  or  BF16    : 14.00G, -0.0050 ppl @ Mistral-7B\n","   0  or  F32     : 26.00G              @ 7B\n","          COPY    : only copy tensors, no quantizing\n"]}],"source":["!sudo apt-get update -y\n","!sudo apt-get install -y build-essential cmake pkg-config libopenblas-dev libomp-dev\n","\n","# go to llama.cpp folder\n","LLAMA_DIR=\"/kaggle/working/llama.cpp\"\n","\n","!echo \"Switching to llama.cpp directory\"\n","%cd $LLAMA_DIR\n","\n","# remove stale cmake leftovers\n","!rm -f CMakeCache.txt\n","!rm -rf CMakeFiles\n","!rm -rf build\n","\n","# create build folder\n","!mkdir build\n","%cd build\n","\n","# run cmake with explicit generator & compilers\n","!cmake .. -G \"Unix Makefiles\" \\\n","  -DCMAKE_BUILD_TYPE=Release \\\n","  -DCMAKE_C_COMPILER=/usr/bin/gcc \\\n","  -DCMAKE_CXX_COMPILER=/usr/bin/g++\n","\n","# build with all CPU cores\n","!cmake --build . --config Release -j$(nproc)\n","\n","# list the binaries\n","!echo \"Built binaries in build/bin:\"\n","!ls -la bin\n","\n","# test the quantizer\n","!./bin/llama-quantize -h\n"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2025-12-02T07:03:13.794139Z","iopub.status.busy":"2025-12-02T07:03:13.793160Z","iopub.status.idle":"2025-12-02T07:03:42.643395Z","shell.execute_reply":"2025-12-02T07:03:42.642740Z","shell.execute_reply.started":"2025-12-02T07:03:13.794102Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["main: build = 7225 (ed3208992)\n","main: built with gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n","main: quantizing '/kaggle/working/llama.cpp/Qwen3-8B-BF16.gguf' to 'Qwen3-8B-Q6_K.gguf' as Q6_K using 8 threads\n","llama_model_loader: loaded meta data with 29 key-value pairs and 338 tensors from /kaggle/working/llama.cpp/Qwen3-8B-BF16.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n","llama_model_loader: - kv   1:                               general.type str              = model\n","llama_model_loader: - kv   2:                     general.sampling.top_k i32              = 20\n","llama_model_loader: - kv   3:                     general.sampling.top_p f32              = 0.800000\n","llama_model_loader: - kv   4:                      general.sampling.temp f32              = 0.700000\n","llama_model_loader: - kv   5:                               general.name str              = Qwen2.5 1.5b Full\n","llama_model_loader: - kv   6:                           general.finetune str              = full\n","llama_model_loader: - kv   7:                           general.basename str              = Qwen2.5\n","llama_model_loader: - kv   8:                         general.size_label str              = 1.5B\n","llama_model_loader: - kv   9:                          qwen2.block_count u32              = 28\n","llama_model_loader: - kv  10:                       qwen2.context_length u32              = 32768\n","llama_model_loader: - kv  11:                     qwen2.embedding_length u32              = 1536\n","llama_model_loader: - kv  12:                  qwen2.feed_forward_length u32              = 8960\n","llama_model_loader: - kv  13:                 qwen2.attention.head_count u32              = 12\n","llama_model_loader: - kv  14:              qwen2.attention.head_count_kv u32              = 2\n","llama_model_loader: - kv  15:                       qwen2.rope.freq_base f32              = 1000000.000000\n","llama_model_loader: - kv  16:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n","llama_model_loader: - kv  17:                          general.file_type u32              = 32\n","llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n","llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2\n","llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = qwen2\n","llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n","llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n","llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n","llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 151645\n","llama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 151643\n","llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 151643\n","llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false\n","llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n","llama_model_loader: - type  f32:  141 tensors\n","llama_model_loader: - type bf16:  197 tensors\n","llama_model_quantize_impl: n_layer_attn = 28, n_layer_recr = 0, pruned_attention_w = 0\n","[   1/ 338]                   output_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[   2/ 338]                    token_embd.weight - [ 1536, 151936,     1,     1], type =   bf16, converting to q6_K .. size =   445.12 MiB ->   182.57 MiB\n","[   3/ 338]                    blk.0.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[   4/ 338]                  blk.0.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[   5/ 338]               blk.0.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[   6/ 338]             blk.0.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[   7/ 338]                    blk.0.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[   8/ 338]                  blk.0.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[   9/ 338]                    blk.0.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  10/ 338]                  blk.0.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[  11/ 338]                blk.0.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  12/ 338]                blk.0.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  13/ 338]                blk.0.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  14/ 338]                  blk.0.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  15/ 338]                    blk.1.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  16/ 338]                  blk.1.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[  17/ 338]               blk.1.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  18/ 338]             blk.1.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[  19/ 338]                    blk.1.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  20/ 338]                  blk.1.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[  21/ 338]                    blk.1.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  22/ 338]                  blk.1.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[  23/ 338]                blk.1.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  24/ 338]                blk.1.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  25/ 338]                blk.1.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  26/ 338]                  blk.1.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  27/ 338]                    blk.2.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  28/ 338]                  blk.2.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[  29/ 338]               blk.2.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  30/ 338]             blk.2.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[  31/ 338]                    blk.2.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  32/ 338]                  blk.2.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[  33/ 338]                    blk.2.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  34/ 338]                  blk.2.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[  35/ 338]                blk.2.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  36/ 338]                blk.2.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  37/ 338]                blk.2.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  38/ 338]                  blk.2.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  39/ 338]                    blk.3.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  40/ 338]                  blk.3.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[  41/ 338]               blk.3.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  42/ 338]             blk.3.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[  43/ 338]                    blk.3.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  44/ 338]                  blk.3.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[  45/ 338]                    blk.3.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  46/ 338]                  blk.3.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[  47/ 338]                blk.3.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  48/ 338]                blk.3.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  49/ 338]                blk.3.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  50/ 338]                  blk.3.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  51/ 338]                    blk.4.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  52/ 338]                  blk.4.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[  53/ 338]               blk.4.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  54/ 338]             blk.4.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[  55/ 338]                    blk.4.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  56/ 338]                  blk.4.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[  57/ 338]                    blk.4.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  58/ 338]                  blk.4.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[  59/ 338]                blk.4.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  60/ 338]                blk.4.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  61/ 338]                blk.4.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  62/ 338]                  blk.4.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  63/ 338]                    blk.5.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  64/ 338]                  blk.5.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[  65/ 338]               blk.5.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  66/ 338]             blk.5.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[  67/ 338]                    blk.5.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  68/ 338]                  blk.5.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[  69/ 338]                    blk.5.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  70/ 338]                  blk.5.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[  71/ 338]                blk.5.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  72/ 338]                blk.5.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  73/ 338]                blk.5.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  74/ 338]                  blk.5.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  75/ 338]                    blk.6.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  76/ 338]                  blk.6.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[  77/ 338]               blk.6.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  78/ 338]             blk.6.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[  79/ 338]                    blk.6.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  80/ 338]                  blk.6.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[  81/ 338]                    blk.6.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  82/ 338]                  blk.6.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[  83/ 338]                blk.6.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  84/ 338]                blk.6.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  85/ 338]                blk.6.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  86/ 338]                  blk.6.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  87/ 338]                    blk.7.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  88/ 338]                  blk.7.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[  89/ 338]               blk.7.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  90/ 338]             blk.7.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[  91/ 338]                    blk.7.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  92/ 338]                  blk.7.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[  93/ 338]                    blk.7.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[  94/ 338]                  blk.7.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[  95/ 338]                blk.7.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  96/ 338]                blk.7.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  97/ 338]                blk.7.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[  98/ 338]                  blk.7.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[  99/ 338]                    blk.8.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 100/ 338]                  blk.8.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 101/ 338]               blk.8.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 102/ 338]             blk.8.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 103/ 338]                    blk.8.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 104/ 338]                  blk.8.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 105/ 338]                    blk.8.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 106/ 338]                  blk.8.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 107/ 338]                blk.8.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 108/ 338]                blk.8.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 109/ 338]                blk.8.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 110/ 338]                  blk.8.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 111/ 338]                    blk.9.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 112/ 338]                  blk.9.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 113/ 338]               blk.9.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 114/ 338]             blk.9.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 115/ 338]                    blk.9.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 116/ 338]                  blk.9.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 117/ 338]                    blk.9.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 118/ 338]                  blk.9.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 119/ 338]                blk.9.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 120/ 338]                blk.9.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 121/ 338]                blk.9.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 122/ 338]                  blk.9.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 123/ 338]                   blk.10.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 124/ 338]                 blk.10.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 125/ 338]              blk.10.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 126/ 338]            blk.10.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 127/ 338]                   blk.10.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 128/ 338]                 blk.10.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 129/ 338]                   blk.10.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 130/ 338]                 blk.10.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 131/ 338]               blk.10.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 132/ 338]               blk.10.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 133/ 338]               blk.10.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 134/ 338]                 blk.10.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 135/ 338]                   blk.11.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 136/ 338]                 blk.11.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 137/ 338]              blk.11.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 138/ 338]            blk.11.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 139/ 338]                   blk.11.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 140/ 338]                 blk.11.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 141/ 338]                   blk.11.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 142/ 338]                 blk.11.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 143/ 338]               blk.11.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 144/ 338]               blk.11.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 145/ 338]               blk.11.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 146/ 338]                 blk.11.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 147/ 338]                   blk.12.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 148/ 338]                 blk.12.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 149/ 338]              blk.12.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 150/ 338]            blk.12.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 151/ 338]                   blk.12.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 152/ 338]                 blk.12.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 153/ 338]                   blk.12.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 154/ 338]                 blk.12.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 155/ 338]               blk.12.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 156/ 338]               blk.12.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 157/ 338]               blk.12.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 158/ 338]                 blk.12.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 159/ 338]                   blk.13.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 160/ 338]                 blk.13.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 161/ 338]              blk.13.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 162/ 338]            blk.13.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 163/ 338]                   blk.13.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 164/ 338]                 blk.13.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 165/ 338]                   blk.13.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 166/ 338]                 blk.13.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 167/ 338]               blk.13.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 168/ 338]               blk.13.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 169/ 338]               blk.13.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 170/ 338]                 blk.13.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 171/ 338]                   blk.14.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 172/ 338]                 blk.14.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 173/ 338]              blk.14.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 174/ 338]            blk.14.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 175/ 338]                   blk.14.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 176/ 338]                 blk.14.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 177/ 338]                   blk.14.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 178/ 338]                 blk.14.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 179/ 338]               blk.14.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 180/ 338]               blk.14.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 181/ 338]               blk.14.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 182/ 338]                 blk.14.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 183/ 338]                   blk.15.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 184/ 338]                 blk.15.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 185/ 338]              blk.15.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 186/ 338]            blk.15.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 187/ 338]                   blk.15.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 188/ 338]                 blk.15.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 189/ 338]                   blk.15.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 190/ 338]                 blk.15.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 191/ 338]               blk.15.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 192/ 338]               blk.15.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 193/ 338]               blk.15.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 194/ 338]                 blk.15.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 195/ 338]                   blk.16.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 196/ 338]                 blk.16.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 197/ 338]              blk.16.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 198/ 338]            blk.16.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 199/ 338]                   blk.16.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 200/ 338]                 blk.16.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 201/ 338]                   blk.16.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 202/ 338]                 blk.16.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 203/ 338]               blk.16.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 204/ 338]               blk.16.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 205/ 338]               blk.16.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 206/ 338]                 blk.16.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 207/ 338]                   blk.17.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 208/ 338]                 blk.17.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 209/ 338]              blk.17.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 210/ 338]            blk.17.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 211/ 338]                   blk.17.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 212/ 338]                 blk.17.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 213/ 338]                   blk.17.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 214/ 338]                 blk.17.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 215/ 338]               blk.17.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 216/ 338]               blk.17.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 217/ 338]               blk.17.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 218/ 338]                 blk.17.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 219/ 338]                   blk.18.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 220/ 338]                 blk.18.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 221/ 338]              blk.18.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 222/ 338]            blk.18.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 223/ 338]                   blk.18.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 224/ 338]                 blk.18.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 225/ 338]                   blk.18.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 226/ 338]                 blk.18.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 227/ 338]               blk.18.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 228/ 338]               blk.18.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 229/ 338]               blk.18.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 230/ 338]                 blk.18.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 231/ 338]                   blk.19.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 232/ 338]                 blk.19.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 233/ 338]              blk.19.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 234/ 338]            blk.19.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 235/ 338]                   blk.19.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 236/ 338]                 blk.19.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 237/ 338]                   blk.19.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 238/ 338]                 blk.19.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 239/ 338]               blk.19.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 240/ 338]               blk.19.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 241/ 338]               blk.19.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 242/ 338]                 blk.19.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 243/ 338]                   blk.20.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 244/ 338]                 blk.20.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 245/ 338]              blk.20.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 246/ 338]            blk.20.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 247/ 338]                   blk.20.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 248/ 338]                 blk.20.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 249/ 338]                   blk.20.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 250/ 338]                 blk.20.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 251/ 338]               blk.20.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 252/ 338]               blk.20.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 253/ 338]               blk.20.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 254/ 338]                 blk.20.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 255/ 338]                   blk.21.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 256/ 338]                 blk.21.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 257/ 338]              blk.21.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 258/ 338]            blk.21.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 259/ 338]                   blk.21.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 260/ 338]                 blk.21.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 261/ 338]                   blk.21.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 262/ 338]                 blk.21.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 263/ 338]               blk.21.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 264/ 338]               blk.21.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 265/ 338]               blk.21.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 266/ 338]                 blk.21.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 267/ 338]                   blk.22.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 268/ 338]                 blk.22.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 269/ 338]              blk.22.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 270/ 338]            blk.22.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 271/ 338]                   blk.22.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 272/ 338]                 blk.22.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 273/ 338]                   blk.22.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 274/ 338]                 blk.22.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 275/ 338]               blk.22.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 276/ 338]               blk.22.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 277/ 338]               blk.22.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 278/ 338]                 blk.22.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 279/ 338]                   blk.23.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 280/ 338]                 blk.23.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 281/ 338]              blk.23.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 282/ 338]            blk.23.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 283/ 338]                   blk.23.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 284/ 338]                 blk.23.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 285/ 338]                   blk.23.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 286/ 338]                 blk.23.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 287/ 338]               blk.23.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 288/ 338]               blk.23.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 289/ 338]               blk.23.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 290/ 338]                 blk.23.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 291/ 338]                   blk.24.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 292/ 338]                 blk.24.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 293/ 338]              blk.24.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 294/ 338]            blk.24.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 295/ 338]                   blk.24.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 296/ 338]                 blk.24.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 297/ 338]                   blk.24.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 298/ 338]                 blk.24.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 299/ 338]               blk.24.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 300/ 338]               blk.24.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 301/ 338]               blk.24.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 302/ 338]                 blk.24.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 303/ 338]                   blk.25.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 304/ 338]                 blk.25.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 305/ 338]              blk.25.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 306/ 338]            blk.25.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 307/ 338]                   blk.25.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 308/ 338]                 blk.25.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 309/ 338]                   blk.25.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 310/ 338]                 blk.25.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 311/ 338]               blk.25.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 312/ 338]               blk.25.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 313/ 338]               blk.25.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 314/ 338]                 blk.25.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 315/ 338]                   blk.26.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 316/ 338]                 blk.26.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 317/ 338]              blk.26.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 318/ 338]            blk.26.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 319/ 338]                   blk.26.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 320/ 338]                 blk.26.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 321/ 338]                   blk.26.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 322/ 338]                 blk.26.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 323/ 338]               blk.26.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 324/ 338]               blk.26.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 325/ 338]               blk.26.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 326/ 338]                 blk.26.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 327/ 338]                   blk.27.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 328/ 338]                 blk.27.attn_k.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 329/ 338]              blk.27.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 330/ 338]            blk.27.attn_output.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 331/ 338]                   blk.27.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 332/ 338]                 blk.27.attn_q.weight - [ 1536,  1536,     1,     1], type =   bf16, converting to q6_K .. size =     4.50 MiB ->     1.85 MiB\n","[ 333/ 338]                   blk.27.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n","[ 334/ 338]                 blk.27.attn_v.weight - [ 1536,   256,     1,     1], type =   bf16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n","[ 335/ 338]               blk.27.ffn_down.weight - [ 8960,  1536,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 336/ 338]               blk.27.ffn_gate.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","[ 337/ 338]               blk.27.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n","[ 338/ 338]                 blk.27.ffn_up.weight - [ 1536,  8960,     1,     1], type =   bf16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n","llama_model_quantize_impl: model size  =  2944.68 MiB\n","llama_model_quantize_impl: quant size  =  1208.10 MiB\n","\n","main: quantize time = 28728.91 ms\n","main:    total time = 28728.91 ms\n"]}],"source":["# !cd /kaggle/working/llama.cpp\n","\n","! ./bin/llama-quantize /kaggle/working/llama.cpp/Qwen3-8B-BF16.gguf Qwen3-8B-Q6_K.gguf Q6_K 8"]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2025-12-02T06:41:27.845133Z","iopub.status.busy":"2025-12-02T06:41:27.844872Z","iopub.status.idle":"2025-12-02T06:41:29.144281Z","shell.execute_reply":"2025-12-02T06:41:29.143495Z","shell.execute_reply.started":"2025-12-02T06:41:27.845114Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":8851005,"sourceId":13892831,"sourceType":"datasetVersion"},{"datasetId":8855172,"sourceId":13899016,"sourceType":"datasetVersion"},{"datasetId":8868082,"sourceId":13917447,"sourceType":"datasetVersion"},{"datasetId":8873104,"sourceId":13924311,"sourceType":"datasetVersion"}],"dockerImageVersionId":31193,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0643dcf5c45c4173803c7cdfd3aa6984":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6b88cd94a8241578e6c219711d79e3b","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8e2eeecb88104837bc8cc83b04f43080","value":1}},"0cd68f23a05c4ff4b339cbb6bb5c39b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab61d4304e1a49afbc2bda4b31f0b128","max":11292,"min":0,"orientation":"horizontal","style":"IPY_MODEL_11a9d879997c4a5ca10260968cca2bf8","value":11292}},"11a9d879997c4a5ca10260968cca2bf8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1292b5ea21884851985b7c37633463fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"19cdea34b8034e839d351340ef808568":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20c15b778d80440597de52726dddb303":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27ea7f41bedb44fe83a43bef6c3ea3aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce93c51c40344d968f54dd0c55a93275","placeholder":"​","style":"IPY_MODEL_abb28edc3b4443fa8aeed99771672630","value":"Generating train split: "}},"2a159994ed144e40acc86bf1599c772d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_27ea7f41bedb44fe83a43bef6c3ea3aa","IPY_MODEL_0643dcf5c45c4173803c7cdfd3aa6984","IPY_MODEL_b6d8b09a1965467dbeca2de508ac89ff"],"layout":"IPY_MODEL_19cdea34b8034e839d351340ef808568"}},"2a6e0dd9885845eb9f7706d2fb7ac30b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e060dea2b91c47fda91f3dfa051e3e60","placeholder":"​","style":"IPY_MODEL_1292b5ea21884851985b7c37633463fc","value":" 1255/1255 [00:03&lt;00:00, 495.50 examples/s]"}},"2ed03703e98a4496a4f7e22f6368befb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_919248b263d7441bab802cba4e193e3a","IPY_MODEL_0cd68f23a05c4ff4b339cbb6bb5c39b7","IPY_MODEL_64043dfe04e544659ba03e5020dab38b"],"layout":"IPY_MODEL_acd32a6e03b54c73bf353da1899f3c41"}},"330d8234e07b44229d2776c90c10ef9e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b4ae42459e2e4bd687331a67bdfc2300","IPY_MODEL_6da872524c564c1e9cc93c9f386fea07","IPY_MODEL_2a6e0dd9885845eb9f7706d2fb7ac30b"],"layout":"IPY_MODEL_20c15b778d80440597de52726dddb303"}},"43da2c25ce3c441c9797ee6bbab76270":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4bc556582e914181be0920ba95ff4bc5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"566447185f3a4e448f576c8391fb2df1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64043dfe04e544659ba03e5020dab38b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4fe64f10fef41268ad622cf7485477e","placeholder":"​","style":"IPY_MODEL_a6b6f43c89b8400f9a0a544afd0b6192","value":" 11292/11292 [00:18&lt;00:00, 431.96 examples/s]"}},"6da872524c564c1e9cc93c9f386fea07":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a80664b5d78d443888f10823779c63b5","max":1255,"min":0,"orientation":"horizontal","style":"IPY_MODEL_af27d4f6dde04e0e852cf112c90b6fee","value":1255}},"6f113d58887e4ce4801f8c27b91e84e8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d73be092fed47a381445281dd39ac84":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e2eeecb88104837bc8cc83b04f43080":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"919248b263d7441bab802cba4e193e3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f113d58887e4ce4801f8c27b91e84e8","placeholder":"​","style":"IPY_MODEL_43da2c25ce3c441c9797ee6bbab76270","value":"Map: 100%"}},"a6b6f43c89b8400f9a0a544afd0b6192":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a6b88cd94a8241578e6c219711d79e3b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"a80664b5d78d443888f10823779c63b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab61d4304e1a49afbc2bda4b31f0b128":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abb28edc3b4443fa8aeed99771672630":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"acd32a6e03b54c73bf353da1899f3c41":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af27d4f6dde04e0e852cf112c90b6fee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b4ae42459e2e4bd687331a67bdfc2300":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_566447185f3a4e448f576c8391fb2df1","placeholder":"​","style":"IPY_MODEL_f5231e3964d647b8a7a8507e40e267ab","value":"Map: 100%"}},"b6d8b09a1965467dbeca2de508ac89ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d73be092fed47a381445281dd39ac84","placeholder":"​","style":"IPY_MODEL_4bc556582e914181be0920ba95ff4bc5","value":" 12547/0 [00:00&lt;00:00, 69894.08 examples/s]"}},"ce93c51c40344d968f54dd0c55a93275":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4fe64f10fef41268ad622cf7485477e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e060dea2b91c47fda91f3dfa051e3e60":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5231e3964d647b8a7a8507e40e267ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":4}
